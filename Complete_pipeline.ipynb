{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import requests \n",
    "import fitz\n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "import textwrap\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '658bf82c83d0bf32f5a03cc5', 'name': 'greasyFinger', 'fullname': 'Narotam N', 'email': 'nnarotam03@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': 1722470399, 'isPro': False, 'avatarUrl': '/avatars/1b909c33015f01cbb8518af26e49900e.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'nlp_a3', 'role': 'write', 'createdAt': '2024-03-30T09:05:09.361Z'}}}\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "\n",
    "# # Initialize the API\n",
    "# api = HfApi()\n",
    "\n",
    "# # Check the currently logged-in user\n",
    "# user_info = api.whoami()\n",
    "\n",
    "# # Print user info\n",
    "# print(user_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str : \n",
    "    clean_txt = text.replace(\"\\n\",\" \").strip()\n",
    "    return clean_txt\n",
    "\n",
    "\n",
    "#     return pages_and_texts\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    print(len(doc))\n",
    "    n = len(doc)\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        if page_number <= n :\n",
    "            text = page.get_text()  # get plain text encoded as UTF-8\n",
    "            text = text_formatter(text)\n",
    "            pages_and_texts.append({\"page_number\": page_number,  \n",
    "                                    \"page_char_count\": len(text),\n",
    "                                    \"page_word_count\": len(text.split(\" \")),\n",
    "                                    \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                    \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                    \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "def chunking(input_list , chunk_size) :\n",
    "    l = [input_list[i : i+ chunk_size] for i in range(0,len(input_list), chunk_size)]\n",
    "    return l\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    \"\"\"Wrap and print text with a given line width.\"\"\"\n",
    "    # Ensure text is a string\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)  # Convert list to string\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "\n",
    "def search_similar_sentences(query_sentence,model,index, k=5):\n",
    "    # Generate embedding for the query sentence\n",
    "    query_embedding = model.encode([query_sentence], convert_to_numpy=True)\n",
    "    \n",
    "    # Ensure query_embedding is 2D\n",
    "    if query_embedding.ndim == 1:\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "    \n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    return distances, indices\n",
    "\n",
    "def print_top_k_results(query,k,distances,indices):\n",
    "    for i in range(k): \n",
    "        print(f\"Distance: {distances[0][i]}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print(\"Text:\")\n",
    "        print_wrapped(pages_and_chunks[indices[0][i]][\"chunks\"])\n",
    "        # Print the page number too so we can reference the textbook further (and check the results)\n",
    "        print(f\"Page number: {pages_and_chunks[indices[0][i]]['page_number']}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "def prompt_formatter(query, context_items, tokenizer, use_dialogue_template=True):\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    # context = \"- \" + \"\\n- \".join([item[\"chunks\"] for item in context_items])\n",
    "    context = \" \".join([item[\"chunks\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"\n",
    "        Based on the following context items, please answer the query.\n",
    "        Context item : \n",
    "        {context}\n",
    "        User query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    if(use_dialogue_template == True) :\n",
    "        # Apply the chat template\n",
    "        prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True)\n",
    "    else : \n",
    "        prompt = tokenizer.apply_chat_template(conversation=base_prompt,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True) \n",
    "    return prompt\n",
    "\n",
    "def ask(query,\n",
    "        model,\n",
    "        index,\n",
    "        pages_and_chunks,\n",
    "        tokenizer,\n",
    "        llm,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True,\n",
    "        use_cache=False):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get just the scores and indices of top related results\n",
    "    # Get relevant resources\n",
    "    scores, indices = search_similar_sentences(query,model,index)\n",
    "        \n",
    "    # Create a list of context items\n",
    "    context_items = [pages_and_chunks[i] for i in indices[0]]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[0][i] # return score back to CPU \n",
    "        \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items,tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"model name\")\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # # Generate an output of tokens\n",
    "    # outputs = llm.generate(**input_ids,\n",
    "    #                              temperature=temperature,\n",
    "    #                              do_sample=True,\n",
    "    #                              max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # # Turn the output tokens into text\n",
    "    # output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    # if format_answer_text:\n",
    "    #     # Replace special tokens and unnecessary help message\n",
    "    #     output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # # Only return the answer without the context items\n",
    "    # if return_answer_only:\n",
    "    #     return output_text\n",
    "    \n",
    "    # return output_text, context_items\n",
    "    outputs = llm.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/ashhar21137/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/ashhar21137/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc96a4ad91c04ef6a94a7a9240e6b433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c817456fea549f6a2c15d57b6027ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True)\n",
    "# In case you want to reduce the maximum length:\n",
    "model.max_seq_length = 8192\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "import faiss\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def process_pdf_and_answer_query(pdf_path, query,model,tokenizer,llm):\n",
    "    # Extract text from the PDF\n",
    "    # Split the file path into root and extension\n",
    "    file_name, file_extension = os.path.splitext(pdf_path)\n",
    "\n",
    "    # Save the file name (without extension) in file_name variable\n",
    "    file_name = file_name\n",
    "\n",
    "    pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "\n",
    "    df = pd.DataFrame(pages_and_texts)\n",
    "    chunk_size=10\n",
    "\n",
    "    \n",
    "    for item in tqdm(pages_and_texts) : \n",
    "        text = item['text']\n",
    "        item[\"sentences\"] = nltk.tokenize.sent_tokenize(text, language='english') \n",
    "\n",
    "        item['page_sentence_count_nltk'] = len(item['sentences'])\n",
    "    \n",
    "    for item in tqdm(pages_and_texts) : \n",
    "        item[\"chunks\"] = chunking(item['sentences'], chunk_size)\n",
    "        item['num_chunks'] = len(item[\"chunks\"])\n",
    "\n",
    "\n",
    "    # Split each chunk into its own item\n",
    "    pages_and_chunks = []\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        for chunk in item[\"chunks\"]:\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "            \n",
    "            # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "            joined_sentence_chunk = \"\".join(chunk).replace(\"  \", \" \").strip()\n",
    "            joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "            chunk_dict[\"chunks\"] = joined_sentence_chunk\n",
    "\n",
    "            # Get stats about the chunk\n",
    "            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "            pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "    df = pd.DataFrame(pages_and_chunks)\n",
    "    min_token_len = 20\n",
    "    df[df[\"chunk_token_count\"] <= min_token_len][\"chunks\"]\n",
    "\n",
    "    # for row in df[df[\"chunk_token_count\"] <= min_token_len].sample(1).iterrows(): \n",
    "    #     print(f'CHunk token count : {row[1][\"chunk_token_count\"]} | text : {row[1][\"chunks\"]}')\n",
    "\n",
    "    pages_and_chunks_over_threshold = df[df[\"chunk_token_count\"] > min_token_len].to_dict(orient=\"records\")\n",
    "\n",
    "    text_chunks = [item[\"chunks\"] for item in pages_and_chunks_over_threshold]\n",
    "\n",
    "\n",
    "    embedding_dim = model.get_sentence_embedding_dimension()\n",
    "    # print(embedding_dim)\n",
    "\n",
    "    index_file_path = f'{file_name}.index'\n",
    "\n",
    "    if not os.path.isfile(index_file_path):\n",
    "        index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "        # Create embeddings one by one on the GPU and add to FAISS index\n",
    "        for item in tqdm(pages_and_chunks_over_threshold):\n",
    "            embeddings = model.encode(item[\"chunks\"], batch_size=32, convert_to_numpy=True)\n",
    "            item[\"embedding\"] = embeddings\n",
    "            # print(embeddings.shape)\n",
    "            if embeddings.ndim == 1:\n",
    "                # If embeddings is 1D, reshape it to 2D\n",
    "                embeddings = np.expand_dims(embeddings, axis=0)\n",
    "            elif embeddings.ndim != 2:\n",
    "                raise ValueError(\"Embeddings should be a 2D array with shape (num_chunks, embedding_dim).\")\n",
    "            index.add(embeddings)\n",
    "\n",
    "        faiss.write_index(index, f'{file_name}.index')\n",
    "\n",
    "    else:\n",
    "      print(\"File already exists. Reading from it\")\n",
    "      index = faiss.read_index(index_file_path)\n",
    "      \n",
    "    # Use the NLP model to answer the query\n",
    "    answer,_=ask(query=query,\n",
    "                            model=model,\n",
    "                            index=index,\n",
    "                            pages_and_chunks=pages_and_chunks,\n",
    "                            tokenizer=tokenizer,\n",
    "                            llm=llm,\n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_path = 'short_stories.pdf'\n",
    "# query = \"Name some stories in the doc?\"\n",
    "\n",
    "# answer,context=process_pdf_and_answer_query(pdf_path,query,model,tokenizer,llm)\n",
    "# print(f\"Answer:\\n\")\n",
    "# print_wrapped(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://44fb3a6569b40d3f68.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://44fb3a6569b40d3f68.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 86.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 177.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8848.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3569.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Reading from it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def wrapped_process_pdf_and_answer_query(pdf_file, query):\n",
    "    return process_pdf_and_answer_query(pdf_file, query, model, tokenizer, llm)\n",
    "\n",
    "\n",
    "# Define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=wrapped_process_pdf_and_answer_query,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF\"),\n",
    "        gr.Textbox(label=\"Enter your query\")\n",
    "        \n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"PDF Query Answering\",\n",
    "    description=\"Upload a PDF and ask a question about its content.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashhar_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
