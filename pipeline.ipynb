{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "import requests \n",
    "import fitz\n",
    "from tqdm import tqdm \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'visual para.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str : \n",
    "    clean_txt = text.replace(\"\\n\",\" \").strip()\n",
    "    return clean_txt\n",
    "\n",
    "\n",
    "#     return pages_and_texts\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    print(len(doc))\n",
    "    n = len(doc)\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        if page_number <= n :\n",
    "            text = page.get_text()  # get plain text encoded as UTF-8\n",
    "            text = text_formatter(text)\n",
    "            pages_and_texts.append({\"page_number\": page_number,  \n",
    "                                    \"page_char_count\": len(text),\n",
    "                                    \"page_word_count\": len(text.split(\" \")),\n",
    "                                    \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                    \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                    \"text\": text})\n",
    "    return pages_and_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:00, 195.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 0,\n",
       "  'page_char_count': 3047,\n",
       "  'page_word_count': 458,\n",
       "  'page_sentence_count_raw': 24,\n",
       "  'page_token_count': 761.75,\n",
       "  'text': 'Image Watermarking Techniques are Brittle: Investigating Visual Paraphrasing for De-Watermarking AI-Generated Images Anonymous submission Forward Diffusion Process Text Caption Generator Black Box An image of Pope Francis smiling and wearing a white jacket, surrounded by a crowd White Box An image of Pope Francis smiling and wearing a white jacket, surrounded by a crowd E UNet . . . . D UNet Denoising Dewatermarked Image  Figure 1: Block diagram of the visual paraphrasing technique illustrating the dewatermarking process. The diagram includes a forward diffusion process for encoding and decoding images to generate visually paraphrased outputs. It features a White Box scenario, where access to prompts is available, allowing direct manipulation of the image using descriptive prompts. In contrast, the Black Box scenario does not have access to prompts, relying on a caption generator (Kosmos 2)(Peng et al. 2023) to interpret and paraphrase the image context indirectly. Abstract With the rise of potent text-to-image generation systems such as Stable Diffusion (Rombach et al. 2022a), Midjourney (Holz 2022), Imagen (Saharia et al. 2022), and DALL-E (Ramesh et al. 2021), the potential for misuse of these tools has esca- lated. In response to the need to curb the circulation of po- tentially misleading visuals created by AI-generated content, companies like Meta have intensified their efforts in imple- menting watermarking techniques on AI-generated images. However, in this paper, we contend that all existing image watermarking methods are fragile and susceptible to being circumvented through visual paraphrasing techniques. We ex- plore two potential scenarios: (i) when we possess the original prompt used to generate an AI-generated image, and (ii) when encountering an image in the wild, potentially watermarked. We empirically assert that in both scenarios, visual paraphrase techniques can effectively remove watermarks from these im- ages. This paper solely critiques current techniques and em- pirically demonstrates the shortcomings of state-of-the-art watermarking techniques for images. AI Generated Image Detection - the Necessity The extraordinary benefits of very large generative AI models such as Stable Diffusion(s), DALL-E(s), Midjourney, Ima- gen, GPT(s), and SORA (to be released) also come with a substantial risk of misuse. The alarm is reflected in the open letter by thousands of researchers and tech leaders in March 2023 for a six-month moratorium on the training of AI sys- tems that are more sophisticated than GPT-4. The central concern noted in the letter (Marcus 2023) is “Should we let machines flood our information channels with propaganda and untruth?”. While individual viewpoints on the notion of a moratorium may vary, the raised concern is significant and warrants attention. With approximately 3.2 billion images and 720,000 hours of video uploaded to social media plat- forms daily (T.J. Thomson 2020) (as of 2020), the need for reliable detection of AI-generated content is more pressing'},\n",
       " {'page_number': 1,\n",
       "  'page_char_count': 5037,\n",
       "  'page_word_count': 714,\n",
       "  'page_sentence_count_raw': 33,\n",
       "  'page_token_count': 1259.25,\n",
       "  'text': 'than ever. The findings of the latest (seventh) evaluation of the Eu- ropean Commission’s Code of Conduct (Commission 2022) that targets the eradication of illegal hate speech online re- veals a decline in companies’ responsiveness. The percent- age of notifications reviewed by companies within 24 hours decreased compared to the two previous monitoring assess- ments, falling from 90.4% in 2020 to 64.4% in 2022. This decline likely reflects the increased accessibility of Gen AI models, leading to a notable influx of AI-generated content on the web. The European Union recently introduced a regu- latory framework for AI (European-Parliament 2023), while the United States has embarked on its initial efforts in for- mulating AI policy (White-House 2023). One of the primary apprehensions among policymakers is that ”Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality.” (Ardi Janjeva and Gausen 2023) Contributions ▶ Introduction of a comprehensive framework for assessing the robustness of watermarking techniques against visual paraphrasing attacks, offering a detailed evaluation of how prevalent watermarking methods can be compromised using state-of-the-art paraphrasing techniques. ▶ Development of a novel set of metrics for evaluating the effectiveness of visual paraphrases in removing watermarks. These metrics are designed to help quantify the ease with which visual paraphrases can eliminate watermarks without degrading the quality of the original image. ▶ Proposal of a new methodology for creating more resilient watermarking technologies. The methodology aims to en- hance the durability of watermarks against visual paraphras- ing attacks, thereby ensuring that watermarked images main- tain their integrity over time. ▶ Establishment of a benchmark dataset for testing the effi- cacy of watermark removal using visual paraphrasing. This dataset provides a standardized set of images with and without watermarks, allowing researchers to rigorously test and compare the performance of different de-watermarking approaches under controlled conditions. In response, companies like Meta along with Deepmind and others, have initiated the development of strategies (Deep- mind 2023) to manage AI-generated content online. These efforts aim to enable the identification and labeling of im- ages sourced from entities like Google, OpenAI, Microsoft, Adobe, Midjourney, and Shutterstock. Various techniques under discussion include: i) Incorporating visible markers on the images, ii) Utilizing invisible watermarks, and iii) Embed- ding metadata within image files. In this paper, we contend that these strategies are inadequate in the era of Generative AI systems. For instance, with the swift advancements in image mask filling systems, detecting visible patches and substituting them using mask filling systems has become more straightforward as shown in figure 2. (a) Original image with visible watermark patch (b) Mask-filled image (first in- stance) (c) Mask-filled image (second instance) (d) Mask-filled image (third in- stance) Figure 2: The figure (Clegg 2024) illustrates how generative AI systems can seamlessly replace visible markers in images. The original image contains distinct visible patches, which are effectively removed and filled using image mask filling techniques. This exemplifies the growing challenge of relying on visible markers for image authenticity in the context of rapidly advancing generative AI capabilities. Similarly, metadata consists of additional tags that can be easily stripped from files using a simple wrapper. Refer to a detailed example in the Appendix for further clarification. This paper exclusively critiques current techniques and empirically illustrates the deficiencies of state-of-the-art (SOTA) methods for AI-generated image detection. Rather than proposing a superior alternative method, this paper serves as a call to action for the scientific community to prioritize the development of more robust AI-generated im- age detection techniques. In this paper, our primary focus is on critiquing water- marking techniques. Although watermarking is primarily a technique originating from the computer vision community, there have been recent attempts to apply watermarking to AI- generated text. These endeavors have faced considerable criti- cism, primarily regarding the ease with which the watermarks can be removed using paraphrase attacks. This paper aligns with the philosophy that visual paraphrasing undermines the effectiveness of watermarking techniques. As such, we em- pirically demonstrate that visual paraphrasing can readily remove watermarks from images. Visual paraphrasing is not yet a widely recognized sub-discipline. However, this paper demonstrates how visual paraphrasing can be accomplished using state-of-the-art text-to-image generation systems. This paper presents a critical assessment, empirically'},\n",
       " {'page_number': 2,\n",
       "  'page_char_count': 5331,\n",
       "  'page_word_count': 794,\n",
       "  'page_sentence_count_raw': 40,\n",
       "  'page_token_count': 1332.75,\n",
       "  'text': 'demonstrating the vulnerability of existing watermarking techniques to visual paraphrase attacks. We do not propose any solutions to address this issue. Rather, this paper serves as a call to action for the scientific community to prioritize the development of more robust watermarking techniques. To the best of our knowledge, no prior work has empirically demonstrated the ability of visual paraphrase to de-watermark images. With this contribution, this paper establishes a scien- tific benchmark for future researchers interested in designing improved watermarking techniques for images generated by Generative AI. Exploring Related Works: Image Watermarking and Detection Methods Watermarking methods have emerged as prominent solu- tions for embedding and detecting ownership information within images. This section delves into the intricacies of these methodologies, exploring their respective strengths, limitations, and applicability in the context of AI-generated image detection. Watermarking Methods Watermarking techniques are broadly classified into two cat- egories: (i) static (i.e., non-learning) watermarking methods and (ii) learning-based watermarking methods. Static Watermarking Methods This method uses Discrete Wavelet Transform (DWT) (Lai and Tsai 2010) to decompose an image into several frequency sub-bands, applies Discrete Cosine Transform (DCT) (Yuan et al. 2020) to each block of some of the sub-bands, and alters certain frequency coefficients of each block via adding a bit of the watermark. The watermarked image is obtained via inverse transform. Learning-based Watermarking Methods Here encoders and decoders (Huynh-The et al. 2019) are neu- ral networks and learn via back-propagation. A watermarking method has three key components: watermark (w), encoder (E), and decoder (D). An encoder takes an image X and watermark w as inputs and produces an watermarked image (Xw). So, Xw = E(X, w) and a decoder takes Xw as an input and produces ˆw = D(Xw). ˆwi = [ ˆwi ≥ τ], where [·] represents the indicator function and τ is a threshold value we decide based on the problem requirements. Tree Ring Watermark The proposed tree-ring watermark- ing (Wen et al. 2023) technique involves embedding the wa- termark into the frequency domain of the initial noise vector using Fast Fourier Transform (FFT), followed by a diffu- sion process applied to the watermarked latent image. To ascertain whether an image has been watermarked, we utilize the inverse diffusion process to reconstruct the latent image, subsequently performing an Inverse Fast Fourier Transform (IFFT). By comparing the L1 distance between the inverted noise vector and the key in the Fourier space of the water- marked area, we determine if the image is watermarked. Any attempt at frequency manipulation or adversarial attack to disrupt this watermark results in loss of image details, ren- dering the image unusable as shown in figure 3 Thus, akin to paraphrasing in text, we explore an approach aiming to retain the image’s essence while not necessarily utilizing the same pixel values. (a) Watermarked (b) JPEG Compressed Prompt: A portrait of a Victorian family, painted in the style of John Singer Sargent. Figure 3: While compression can strip away watermarks, it simultaneously degrades the overall image quality. The comparative images demonstrate the loss of clarity and detail, highlighting the trade-off between watermark removal and maintaining high image fidelity. Stable Signature The Stable Signature (Fernandez et al. 2023) method represents a pioneering technique in the do- main of watermarking images generated through latent diffu- sion models (LDMs) (Rombach et al. 2022b). This method hinges on several crucial insights. LDMs produce images by progressively denoising a latent representation of the image. Watermarking is executed by subtly altering the latent repre- sentation in a manner imperceptible to human observation but discernible by a pretrained watermark extractor network. The essence of the Stable Signature technique revolves around refining the LDM decoder to yield images that manifest a predetermined signature when scrutinized by the watermark extractor network. This involves the minimization of a loss function that amalgamates the reconstruction loss and the watermark loss, wherein the former assesses the variance be- tween the generated image and the target image, and the latter quantifies the discrepancy between the signature of the gener- ated image and the desired watermark signature. The balance between these two aspects is regulated by a hyperparameter denoted as λ. In essence, the Stable Signature approach operates as fol- lows: First, a watermark extractor network is trained to recog- nize a particular watermark within images. Subsequently, the LDM decoder is meticulously fine-tuned to produce images that display the predefined signature when passed through the watermark extractor network. To generate a watermarked image, one simply samples a latent representation from the LDM and decodes it employing the finely calibrated decoder. Standard Training We use SGD (Zhang et al. 2018) to optimize the loss function: Pn i=1 li(D(Xw), w). Adversarial Training We can apply adversarial (Wen and Aydore 2019) training in order to improve robustness against post-processing.'},\n",
       " {'page_number': 3,\n",
       "  'page_char_count': 4985,\n",
       "  'page_word_count': 806,\n",
       "  'page_sentence_count_raw': 41,\n",
       "  'page_token_count': 1246.25,\n",
       "  'text': 'Generated Image Watermarked Image Difference Watermarked Image Visual Paraphrased Difference Prompt: A portrait of a Victorian family, painted in the style of John Singer Sargent. Figure 4: The figure demonstrates the effects of watermark embedding and visual paraphrasing on images. The first row presents the individual pixel-wise differences between the original generated image and the watermarked image, high- lighting the specific pixels modified by the watermarking process. The second row illustrates the individual pixel-wise differences between the watermarked image and its visually paraphrased version, indicating the pixels initially impacted by the watermark embedding that were subsequently altered through visual paraphrasing. ZoDiac Watermarking ZoDiac(Zhang et al. 2024) is a zero-shot watermarking technique that leverages pre-trained diffusion models to embed watermarks into images while maintaining visual similarity between the watermarked and original images. The method comprises three main steps: I. Latent Vector Initialization: A trainable vector is initial- ized to be used by the Stable Diffusion model to reproduce the original image x0. The original image x0 undergoes the DDIM (Song, Meng, and Ermon 2022) inversion pro- cess to generate the latent vector ZT . II. Watermark Encoding: The latent vector ZT is trans- formed into its Fourier space, where a concentric ring-like watermark is embedded. This watermark is similar to the one used in tree-ring watermarking. To ensure that the final watermarked image ˆx0 closely resembles the origi- nal image, ZoDiac iteratively refines the latent vector ZT using a custom reconstruction loss. III. Adaptive Image Enhancement: Once the watermarked image ˆx0 is generated, its visual quality is enhanced by adaptively mixing it with the original image x0 to meet a desired image quality threshold. Unlike tree-ring watermarking, ZoDiac can be used to water- mark existing images. SynthID SynthID (Deepmind 2023) is a toolkit from Google DeepMind that watermarks AI-generated content. It utilizes a data-driven watermarking approach, embedding an imperceptible mark during AI-generated content (AIGC) creation. This mark, robust to post-processing edits, persists across different modalities like images, videos, text, and au- dio. It watermarks AI-generated images by training two neu- ral networks: one to imperceptibly modify pixels, creating a unique pattern, and another to detect this pattern even af- ter edits, ensuring the image can be identified as AI-made despite manipulations. However, benchmarking SynthID is not feasible due to the unavailability of its code. This watermarking technique is internally developed by Google for all their AI-generated con- tent, including images, and they have not disclosed the code or provided an API for external testing. Therefore, compar- isons with SynthID are omitted from this study, as we lack the necessary access to implement or evaluate its performance. Watermarking Detection Methods VS: We need to make it more explicit why we are talking about the watermark detection techniques... if we claim the watermarking techniques are brittle, are we also saying that the detection techniques are not good enough and cant handle minor perturbations... or are we saying that the watermarking itself is flawed. Also are the detection techniques agnostic of the watermarking technique itself? The messaging should be clear and the experiments designed accordingly. Single-tail Detector (Fernandez et al. 2023; Yu et al. 2021; Zhao et al. 2023) An image is detected as AI- generated if bitwise accuracy (BA) between w and ˆw is greater than a threshold value. We say an image is AI- generated if BA(w, ˆw) ≥ τ. We need to select the τ such that false positive rate (FPR) is as low as possible. Suppose BA(w, ˆw) = m n , where m is the number of matched bits and n is the total number of bits. There are m successes out of n and each success or failure has a probability of 0.5. FPR can be measured as the binomial distribution B(n, 0.5). FPR is calculated as follows: FPR(τ) = P(BA(w, ˆw) > τ) = P(m > nτ) = n X i=⌈nτ⌉ \\x12n i \\x13 \\x121 2 \\x13n (1) Double-tail Detector The single-tail detector can fail eas- ily if a small perturbation is added with watermarked images. The double-tail detector (Jiang, Zhang, and Gong 2023) is useful to detect perturbed images. The watermark decoded from original images has bitwise accuracy 0.5, while the watermark decoded from watermarked images has bitwise ac- curacy close to 1. So, if the bitwise accuracy of the watermark decoded from an image is much below than 0.5, the image is considered as a perturbed image. The FPR is calculated as follows: FPR(τ) = P(BA(w, ˆw) > τ) or P(BW(w, ˆw) < 1 − τ) = P(m > nτ) + P(m < n − nτ) = n X i=⌈nτ⌉   n i ! \\x121 2 \\x13n + ⌊n−nτ⌋ X i=1   n i ! \\x121 2 \\x13n (2) Visual Paraphrasing In the realm of AI-generated image detection, visual para- phrasing is a crucial method for confirming the authenticity'},\n",
       " {'page_number': 4,\n",
       "  'page_char_count': 2580,\n",
       "  'page_word_count': 415,\n",
       "  'page_sentence_count_raw': 22,\n",
       "  'page_token_count': 645.0,\n",
       "  'text': 'Tree Ring Stable Signature Figure 5: This figure shows the variation of CMMD (Jayasumana et al. 2024) and detectability of visual paraphrases with respect to strength and guidance scale. The images were watermarked using Tree Ring Watermarking (Wen et al. 2023) and Stable Signature (Fernandez et al. 2023). VS: We need justification of why we only benchmark these two and how they are representative of the broader class on watermarking methods. In lit review we mention SythiID and Zodiac but dont benchmark them? original image s=0.2 s=0.3 s=0.4 s=0.5 s=0.6 s=0.7 Prompt: Potrait of a Labrador in the style of Van Gogh Figure 6: Varying strength for content injection: The intensity of noise injected into the content is varied which impacts both the preservation of layout semantics and the fusion of prompt semantics. and consistency of digital images. In this procedure, the orig- inal context and content are preserved while producing an image that is semantically comparable. At the core of visual paraphrasing lies the image-to-image diffusion process (Gilboa, Sochen, and Zeevi 2002). This technique, employed in generative models, transforms images while maintaining their underlying structure and semantic in- formation. The diffusion process involves two key stages: the forward diffusion process and the reverse diffusion process. In the forward diffusion process, an image is gradually corrupted by adding noise, eventually reaching a state of complete noise. This simulates the progressive destruction of the image’s structure, making it harder to reconstruct the original image at each step. Mathematically, this process is described as follows: xt = √αtxt−1 + √ 1 − αtϵt, (3) where xt is the image at time step t, αt is a noise scaling factor, and ϵt is the noise sampled from a Gaussian distribu- tion. In the reverse diffusion process, the model attempts to remove the noise step by step, reconstructing the original image from the noisy version. This is achieved using a learned denoising function ϵθ: xt−1 = 1 √αt \\x00xt − √ 1 − αtϵθ(xt, t) \\x01 . (4) This iterative denoising process continues until the model retrieves an image that is visually and semantically similar to the original input. The number of inference steps, denoted as T, plays a critical role in this process. A higher number of steps generally allows for finer reconstruction, leading to higher quality images, but at the cost of increased computa- tional complexity and time. There are two distinct approaches to visual paraphrasing: (i) White Box and (ii) Black Box. Each method offers unique'},\n",
       " {'page_number': 5,\n",
       "  'page_char_count': 4011,\n",
       "  'page_word_count': 580,\n",
       "  'page_sentence_count_raw': 29,\n",
       "  'page_token_count': 1002.75,\n",
       "  'text': 'Figure 7: This figure illustrates the image-to-image diffusion process (Gilboa, Sochen, and Zeevi 2002). The top row demonstrates the forward diffusion process, where the original image progressively becomes more noisy. The bottom row shows the denoising process, where noise is incrementally removed from the noisy image, guided by text conditioning to generate the final, Visual Paraphrased image. advantages and is suited to different use cases. The following sections will explore the specifics of each approach, detailing their methodologies and applications. White Box Visual Paraphrase Attack In the White Box scenario, access to the original prompt and image is avail- able, enabling a direct approach to visual paraphrasing. Here, the prompt serves as text conditioning for the watermarked image, providing essential context for regenerating the non- watermarked image. Leveraging advanced techniques such as image-to-image diffusion models, the watermarked image is processed in conjunction with the prompt, facilitating the generation of a visually indistinguishable counterpart. This approach ensures the preservation of semantic consistency while effectively removing watermarks from the image. Black Box Visual Paraphrase Attack Conversely, in the Black Box scenario, access to the original prompt is unavail- able, necessitating an alternative approach to visual para- phrasing. In this context, the image is passed through an im- age caption generator, such as KOSMOS 2(Peng et al. 2023), to obtain a textual representation of its content. The gener- ated caption serves as text conditioning for the watermarked image, facilitating its processing through image-to-image dif- fusion models. By utilizing the extracted textual context as a guiding force, the diffusion model reconstructs the image while preserving its semantic content, effectively achieving visual paraphrasing even in the absence of direct access to the original prompt. AD: need a diagram here Performance on De-Watermarking After obtaining the visual paraphrased image, we evaluate the effectiveness of de-watermarking by passing the im- age through various watermark extraction techniques. We then compare the originally embedded message with the ex- tracted message, assessing the accuracy of bit precisions. The visual paraphrased images are further evaluated based on their Frechet Inception Distance (FID) (Nunn, Khadivi, and Samavi 2021) scores in relation to the reference images to determine the best quality output. Our findings indicate that watermarking techniques are particularly brittle when subjected to visual paraphrasing. The transformation processes inherent in visual paraphrasing, especially under higher strength settings, often disrupt the embedded watermarks, leading to challenges in accurately extracting the original messages as shown in figure 11. How- ever, increasing the strength excessively deviates the image from the original, thereby increasing the CMMD score (Jaya- sumana et al. 2024). Conversely, the guidance scale value needs to be sufficiently high to remove the watermark but increases the CMMD. A too low guidance scale value also raises the CMMD without effectively removing the water- mark. Therefore, an optimal value of strength and guidance scale must be identified. By selecting the visual paraphrased image with the optimal FID and CMMD , we aim to balance the preservation of image quality and the integrity of the embedded watermark information. Information Loss due to Visual Hallucination During the process of visual paraphrasing using image-to- image diffusion (Gilboa, Sochen, and Zeevi 2002) with text conditioning, there exists the possibility of information loss from the original image. This loss occurs due to the trans- formation applied to the reference image, which introduces noise and alters its visual characteristics. However, it is es- sential to strike a balance between minimizing information loss and maintaining the quality of the generated image. This'},\n",
       " {'page_number': 6,\n",
       "  'page_char_count': 2700,\n",
       "  'page_word_count': 406,\n",
       "  'page_sentence_count_raw': 16,\n",
       "  'page_token_count': 675.0,\n",
       "  'text': 'Watermarking Method Watermark Detection Rate (η) Pre Attack Post Attack Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase COCO (Lin et al. 2015) DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - Table 1: Watermark Detection Rates (η) for Different Techniques and Attacks delicate equilibrium can be achieved by controlling the hy- perparameters of the image-to-image diffusion model. (a) Watermarked (b) Visual Paraphrase (c) Watermarked (d) Visual Paraphrase Figure 9: Comparison of watermarked images (a, c) with their visual paraphrased counterparts (b, d). Each colored box (red, yellow, and blue) represents a one-to-one comparison of the same region in the original and paraphrased images, highlighting how visual paraphrasing alters specific elements within these regions, resulting in information loss. One crucial hyperparameter is the strength parameter, which determines the extent of transformation applied to the reference image. A higher strength value results in more significant noise being added to the image, thereby deviating further from the original. Conversely, a lower strength value preserves the image’s fidelity to the original, reducing the risk of information loss. By carefully adjusting the strength parameter, researchers can regulate the amount of noise in- troduced during the diffusion process, thereby mitigating potential information loss while maintaining image quality. Another vital hyperparameter is the guidance scale, which influences the model’s adherence to the text prompt during image generation. A higher guidance scale encourages the model to prioritize fidelity to the prompt, potentially sacrific- ing image quality in the process. Conversely, a lower guid- ance scale allows for more flexibility in image generation, potentially leading to higher-quality results at the expense of strict adherence to the text prompt. By fine-tuning the guid- ance scale, researchers can strike an optimal balance between textual fidelity and image quality, thereby mitigating infor- mation loss while ensuring the generated image’s coherence with the provided prompt. Human evaluation on Visual Paraphrase Figure 10: Heatmap of MOS scores with 500 assessed sam- ples for each category. Conclusion In conclusion, the imperative for robust AI-generated image detection mechanisms has never been more pressing. The proliferation of highly realistic generative models poses sig- nificant challenges to content ownership and the dissemina- tion of misinformation. By exploring watermarking methods, visual paraphrasing scenarios, and the control of information'},\n",
       " {'page_number': 7,\n",
       "  'page_char_count': 699,\n",
       "  'page_word_count': 144,\n",
       "  'page_sentence_count_raw': 3,\n",
       "  'page_token_count': 174.75,\n",
       "  'text': 'Watermarked Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase (Ours) η = 1 η = 0.989 η = 0.841 η = 0.624 η = 0.671 η = 0.263 η = 1 η = 0.991 η = 0.813 η = 0.611 η = 0.633 η = 0.334 η = 1 η = 0.984 η = 0.837 η = 0.656 η = 0.603 η = 0.297 η = 1 η = 0.994 η = 0.784 η = 0.609 η = 0.579 η = 0.273 η = 1 η = 0.997 η = 0.759 η = 0.702 η = 0.682 η = 0.311 Table 2: The figure shows watermarked images, images under various attacks, and our visual paraphrase method. The attacks include Brightness adjustment, Rotation, JPEG Compression, and Gaussian Noise, along with our Visual Paraphrase method. η comparisons, representing watermark detection score (bit accuracy), are also provided.'},\n",
       " {'page_number': 8,\n",
       "  'page_char_count': 5741,\n",
       "  'page_word_count': 791,\n",
       "  'page_sentence_count_raw': 96,\n",
       "  'page_token_count': 1435.25,\n",
       "  'text': 'loss, this paper has underscored the critical role of techno- logical innovation in safeguarding the integrity of digital imagery. Moving forward, continued research and develop- ment in AI-generated image detection promise to fortify the foundations of a trustworthy digital ecosystem, empowering individuals and organizations to navigate the complexities of the information age with confidence and discernment. Ethical Considerations The development of visual paraphrasing methods, particularly those capable of dewatermarking state-of-the-art watermark- ing techniques, necessitates careful ethical considerations. While the primary intent behind this research is to advance the field of image processing and provide a deeper under- standing of watermarking resilience, it is crucial to acknowl- edge the potential for misuse. This study is conducted with the sole purpose of academic exploration and innovation, aiming to enhance the robustness of watermarking methods by identifying their vulnerabilities and improving overall se- curity. Our work is intended to contribute positively to the field, encouraging the development of more sophisticated and tamper-resistant watermarking techniques. Despite the benign intentions, the capabilities demon- strated by visual paraphrasing could be exploited for unethi- cal purposes, such as unauthorized removal of watermarks from copyrighted images, undermining the efforts of con- tent creators and rights holders to protect their intellectual property. To mitigate such risks, we emphasize responsible disclosure of our findings to stakeholders in the watermarking community, including researchers, developers, and content protection agencies, to foster collaborative improvements in watermarking technologies. Detailed methodologies and tools developed in this research will be restricted to aca- demic and professional entities with legitimate interests in advancing watermarking techniques. Furthermore, we advo- cate for the establishment of ethical guidelines for the use of visual paraphrasing tools, outlining acceptable uses such as research, education, and security testing, while explicitly prohibiting applications that infringe on intellectual property rights. Through these measures, we are committed to con- ducting our research in accordance with the highest ethical standards, ensuring that our work aligns with broader societal values and legal frameworks. References Ardi Janjeva, S. M. A. K., Alexander Harris; and Gausen, A. 2023. The Rapid Rise of Gen- erative AI: Assessing risks to safety and secu- rity. https://cetas.turing.ac.uk/sites/default/files/2023- 12/cetas research report - the rapid rise of generative ai - 2023.pdf. Clegg, N. 2024. Labeling AI-Generated Images on Facebook, Instagram and Threads — Meta — about.fb.com. https://about.fb.com/news/2024/02/labeling- ai-generated-images-on-facebook-instagram-and-threads/. Commission, E. 2022. EU Code of Conduct against online hate speech: latest evaluation shows slowdown in progress. Deepmind. 2023. dentifying AI-generated images with Syn- thID. European-Parliament. 2023. Proposal for a regulation of the European Parliament and of the Council laying down har- monised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts. Fernandez, P.; Couairon, G.; J´egou, H.; Douze, M.; and Furon, T. 2023. The Stable Signature: Rooting Watermarks in Latent Diffusion Models. arXiv:2303.15435. Gilboa, G.; Sochen, N.; and Zeevi, Y. 2002. Forward-and- backward diffusion processes for adaptive image enhance- ment and denoising. IEEE Transactions on Image Processing, 11(7): 689–703. Holz, D. 2022. Midjouney Inc. https://www.midjourney. com/. Huynh-The, T.; Hua, C.-H.; Tu, N. A.; and Kim, D.-S. 2019. Robust Image Watermarking Framework Powered by Convo- lutional Encoder-Decoder Network. In 2019 Digital Image Computing: Techniques and Applications (DICTA), 1–7. Jayasumana, S.; Ramalingam, S.; Veit, A.; Glasner, D.; Chakrabarti, A.; and Kumar, S. 2024. Rethinking FID: Towards a Better Evaluation Metric for Image Generation. arXiv:2401.09603. Jiang, Z.; Zhang, J.; and Gong, N. Z. 2023. Evading Water- mark based Detection of AI-Generated Content. In ACM Con- ference on Computer and Communications Security (CCS). Lai, C.-C.; and Tsai, C.-C. 2010. Digital image watermark- ing using discrete wavelet transform and singular value de- composition. IEEE Transactions on instrumentation and measurement, 59(11): 3060–3063. Lin, T.-Y.; Maire, M.; Belongie, S.; Bourdev, L.; Girshick, R.; Hays, J.; Perona, P.; Ramanan, D.; Zitnick, C. L.; and Doll´ar, P. 2015. Microsoft COCO: Common Objects in Context. arXiv:1405.0312. Marcus, G. 2023. Pause Giant AI Experiments: An Open Letter. Nunn, E. J.; Khadivi, P.; and Samavi, S. 2021. Compound Frechet Inception Distance for Quality Assessment of GAN Created Images. arXiv:2106.08575. Peng, Z.; Wang, W.; Dong, L.; Hao, Y.; Huang, S.; Ma, S.; and Wei, F. 2023. Kosmos-2: Grounding Multimodal Large Language Models to the World. arXiv:2306.14824. Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Radford, A.; Chen, M.; and Sutskever, I. 2021. Zero-Shot Text-to- Image Generation. arXiv:2102.12092. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om- mer, B. 2022a. High-Resolution Image Synthesis with Latent Diffusion Models. arXiv:2112.10752. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om- mer, B. 2022b. High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), 10684–10695. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E.; Ghasemipour, S. K. S.; Ayan, B. K.; Mahdavi, S. S.;'},\n",
       " {'page_number': 9,\n",
       "  'page_char_count': 1715,\n",
       "  'page_word_count': 250,\n",
       "  'page_sentence_count_raw': 49,\n",
       "  'page_token_count': 428.75,\n",
       "  'text': 'Lopes, R. G.; Salimans, T.; Ho, J.; Fleet, D. J.; and Norouzi, M. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. arXiv:2205.11487. Song, J.; Meng, C.; and Ermon, S. 2022. Denoising Diffusion Implicit Models. arXiv:2010.02502. T.J. Thomson, P. D., Daniel Angus. 2020. 3.2 billion images and 720,000 hours of video are shared online daily. Can you sort real from fake? Wen, B.; and Aydore, S. 2019. ROMark: A Ro- bust Watermarking System Using Adversarial Training. arXiv:1910.01221. Wen, Y.; Kirchenbauer, J.; Geiping, J.; and Goldstein, T. 2023. Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust. arXiv:2305.20030. White-House. 2023. Blueprint for an AI Bill of Rights: Mak- ing Automated Systems Work For the American People. Yu, N.; Skripniuk, V.; Abdelnabi, S.; and Fritz, M. 2021. Arti- ficial fingerprinting for generative models: Rooting deepfake attribution in training data. In Proceedings of the IEEE/CVF International conference on computer vision, 14448–14457. Yuan, Z.; Liu, D.; Zhang, X.; and Su, Q. 2020. New im- age blind watermarking method based on two-dimensional discrete cosine transform. Optik, 204: 164152. Zhang, C.; Liao, Q.; Rakhlin, A.; Miranda, B.; Golowich, N.; and Poggio, T. 2018. Theory of Deep Learning IIb: Optimization Properties of SGD. arXiv:1801.02254. Zhang, L.; Liu, X.; Martin, A. V.; Bearfield, C. X.; Brun, Y.; and Guan, H. 2024. Robust Image Watermarking using Stable Diffusion. arXiv preprint arXiv:2401.04247. Zhao, X.; Zhang, K.; Wang, Y.-X.; and Li, L. 2023. Genera- tive Autoencoders as Watermark Attackers: Analyses of Vul- nerabilities and Threats. arXiv preprint arXiv:2306.01953. Appendix'},\n",
       " {'page_number': 10,\n",
       "  'page_char_count': 660,\n",
       "  'page_word_count': 167,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 165.0,\n",
       "  'text': 'Watermarking Method Watermark Detection Rate (η) Pre Attack Post Attack Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase COCO DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - DiffusionDB DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - WikiArt DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - Table 3: Watermark Detection Rates η for Different Techniques and Attacks'},\n",
       " {'page_number': 11,\n",
       "  'page_char_count': 168,\n",
       "  'page_word_count': 46,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 42.0,\n",
       "  'text': 'η = 1 η = 0.65 η = 0.54 η = 1 η = 0.57 η = 0.43 η = 1 η = 0.41 η = 0.51 η = 1 η = 0.51 η = 0.38 Figure 8: Examples of Visual Paraphrasing and their detectability scores'},\n",
       " {'page_number': 12,\n",
       "  'page_char_count': 484,\n",
       "  'page_word_count': 75,\n",
       "  'page_sentence_count_raw': 7,\n",
       "  'page_token_count': 121.0,\n",
       "  'text': 'Tree Ring Stable Signature Zodiac Gauss Shadding DctDwdSVD HiDDen Figure 11: For Zodiac , Gaushadding , DctDwdSVD , HiDDen this graphs are just placeholders. Real graphs needs to be added. This figure shows the variation of CMMD (Jayasumana et al. 2024) and detectability of visual paraphrases with respect to strength and guidance scale. The images were watermarked using Tree Ring Watermarking (Wen et al. 2023), Stable Signature (Fernandez et al. 2023), Zodiac, and Gauss Shadding.'},\n",
       " {'page_number': 13,\n",
       "  'page_char_count': 826,\n",
       "  'page_word_count': 161,\n",
       "  'page_sentence_count_raw': 5,\n",
       "  'page_token_count': 206.5,\n",
       "  'text': 'Watermarked Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase (Ours) η = 1 η = 0.989 η = 0.841 η = 0.624 η = 0.671 η = 0.263 η = 1 η = 0.991 η = 0.813 η = 0.611 η = 0.633 η = 0.334 η = 1 η = 0.984 η = 0.837 η = 0.656 η = 0.603 η = 0.297 η = 1 η = 0.994 η = 0.784 η = 0.609 η = 0.579 η = 0.273 η = 1 η = 0.997 η = 0.759 η = 0.702 η = 0.682 η = 0.311 Table 4: Placeholder, these are stable signature examples. need to put tree ring, ZoDiac, Gaussian Shading, dwtdctsvd, HiDDen examples. The figure shows watermarked images, images under various attacks, and our visual paraphrase method. The attacks include Brightness adjustment, Rotation, JPEG Compression, and Gaussian Noise, along with our Visual Paraphrase method. η comparisons, representing watermark detection score (bit accuracy), are also provided.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 4,\n",
       "  'page_char_count': 2580,\n",
       "  'page_word_count': 415,\n",
       "  'page_sentence_count_raw': 22,\n",
       "  'page_token_count': 645.0,\n",
       "  'text': 'Tree Ring Stable Signature Figure 5: This figure shows the variation of CMMD (Jayasumana et al. 2024) and detectability of visual paraphrases with respect to strength and guidance scale. The images were watermarked using Tree Ring Watermarking (Wen et al. 2023) and Stable Signature (Fernandez et al. 2023). VS: We need justification of why we only benchmark these two and how they are representative of the broader class on watermarking methods. In lit review we mention SythiID and Zodiac but dont benchmark them? original image s=0.2 s=0.3 s=0.4 s=0.5 s=0.6 s=0.7 Prompt: Potrait of a Labrador in the style of Van Gogh Figure 6: Varying strength for content injection: The intensity of noise injected into the content is varied which impacts both the preservation of layout semantics and the fusion of prompt semantics. and consistency of digital images. In this procedure, the orig- inal context and content are preserved while producing an image that is semantically comparable. At the core of visual paraphrasing lies the image-to-image diffusion process (Gilboa, Sochen, and Zeevi 2002). This technique, employed in generative models, transforms images while maintaining their underlying structure and semantic in- formation. The diffusion process involves two key stages: the forward diffusion process and the reverse diffusion process. In the forward diffusion process, an image is gradually corrupted by adding noise, eventually reaching a state of complete noise. This simulates the progressive destruction of the image’s structure, making it harder to reconstruct the original image at each step. Mathematically, this process is described as follows: xt = √αtxt−1 + √ 1 − αtϵt, (3) where xt is the image at time step t, αt is a noise scaling factor, and ϵt is the noise sampled from a Gaussian distribu- tion. In the reverse diffusion process, the model attempts to remove the noise step by step, reconstructing the original image from the noisy version. This is achieved using a learned denoising function ϵθ: xt−1 = 1 √αt \\x00xt − √ 1 − αtϵθ(xt, t) \\x01 . (4) This iterative denoising process continues until the model retrieves an image that is visually and semantically similar to the original input. The number of inference steps, denoted as T, plays a critical role in this process. A higher number of steps generally allows for finer reconstruction, leading to higher quality images, but at the cost of increased computa- tional complexity and time. There are two distinct approaches to visual paraphrasing: (i) White Box and (ii) Black Box. Each method offers unique'},\n",
       " {'page_number': 13,\n",
       "  'page_char_count': 826,\n",
       "  'page_word_count': 161,\n",
       "  'page_sentence_count_raw': 5,\n",
       "  'page_token_count': 206.5,\n",
       "  'text': 'Watermarked Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase (Ours) η = 1 η = 0.989 η = 0.841 η = 0.624 η = 0.671 η = 0.263 η = 1 η = 0.991 η = 0.813 η = 0.611 η = 0.633 η = 0.334 η = 1 η = 0.984 η = 0.837 η = 0.656 η = 0.603 η = 0.297 η = 1 η = 0.994 η = 0.784 η = 0.609 η = 0.579 η = 0.273 η = 1 η = 0.997 η = 0.759 η = 0.702 η = 0.682 η = 0.311 Table 4: Placeholder, these are stable signature examples. need to put tree ring, ZoDiac, Gaussian Shading, dwtdctsvd, HiDDen examples. The figure shows watermarked images, images under various attacks, and our visual paraphrase method. The attacks include Brightness adjustment, Rotation, JPEG Compression, and Gaussian Noise, along with our Visual Paraphrase method. η comparisons, representing watermark detection score (bit accuracy), are also provided.'},\n",
       " {'page_number': 3,\n",
       "  'page_char_count': 4985,\n",
       "  'page_word_count': 806,\n",
       "  'page_sentence_count_raw': 41,\n",
       "  'page_token_count': 1246.25,\n",
       "  'text': 'Generated Image Watermarked Image Difference Watermarked Image Visual Paraphrased Difference Prompt: A portrait of a Victorian family, painted in the style of John Singer Sargent. Figure 4: The figure demonstrates the effects of watermark embedding and visual paraphrasing on images. The first row presents the individual pixel-wise differences between the original generated image and the watermarked image, high- lighting the specific pixels modified by the watermarking process. The second row illustrates the individual pixel-wise differences between the watermarked image and its visually paraphrased version, indicating the pixels initially impacted by the watermark embedding that were subsequently altered through visual paraphrasing. ZoDiac Watermarking ZoDiac(Zhang et al. 2024) is a zero-shot watermarking technique that leverages pre-trained diffusion models to embed watermarks into images while maintaining visual similarity between the watermarked and original images. The method comprises three main steps: I. Latent Vector Initialization: A trainable vector is initial- ized to be used by the Stable Diffusion model to reproduce the original image x0. The original image x0 undergoes the DDIM (Song, Meng, and Ermon 2022) inversion pro- cess to generate the latent vector ZT . II. Watermark Encoding: The latent vector ZT is trans- formed into its Fourier space, where a concentric ring-like watermark is embedded. This watermark is similar to the one used in tree-ring watermarking. To ensure that the final watermarked image ˆx0 closely resembles the origi- nal image, ZoDiac iteratively refines the latent vector ZT using a custom reconstruction loss. III. Adaptive Image Enhancement: Once the watermarked image ˆx0 is generated, its visual quality is enhanced by adaptively mixing it with the original image x0 to meet a desired image quality threshold. Unlike tree-ring watermarking, ZoDiac can be used to water- mark existing images. SynthID SynthID (Deepmind 2023) is a toolkit from Google DeepMind that watermarks AI-generated content. It utilizes a data-driven watermarking approach, embedding an imperceptible mark during AI-generated content (AIGC) creation. This mark, robust to post-processing edits, persists across different modalities like images, videos, text, and au- dio. It watermarks AI-generated images by training two neu- ral networks: one to imperceptibly modify pixels, creating a unique pattern, and another to detect this pattern even af- ter edits, ensuring the image can be identified as AI-made despite manipulations. However, benchmarking SynthID is not feasible due to the unavailability of its code. This watermarking technique is internally developed by Google for all their AI-generated con- tent, including images, and they have not disclosed the code or provided an API for external testing. Therefore, compar- isons with SynthID are omitted from this study, as we lack the necessary access to implement or evaluate its performance. Watermarking Detection Methods VS: We need to make it more explicit why we are talking about the watermark detection techniques... if we claim the watermarking techniques are brittle, are we also saying that the detection techniques are not good enough and cant handle minor perturbations... or are we saying that the watermarking itself is flawed. Also are the detection techniques agnostic of the watermarking technique itself? The messaging should be clear and the experiments designed accordingly. Single-tail Detector (Fernandez et al. 2023; Yu et al. 2021; Zhao et al. 2023) An image is detected as AI- generated if bitwise accuracy (BA) between w and ˆw is greater than a threshold value. We say an image is AI- generated if BA(w, ˆw) ≥ τ. We need to select the τ such that false positive rate (FPR) is as low as possible. Suppose BA(w, ˆw) = m n , where m is the number of matched bits and n is the total number of bits. There are m successes out of n and each success or failure has a probability of 0.5. FPR can be measured as the binomial distribution B(n, 0.5). FPR is calculated as follows: FPR(τ) = P(BA(w, ˆw) > τ) = P(m > nτ) = n X i=⌈nτ⌉ \\x12n i \\x13 \\x121 2 \\x13n (1) Double-tail Detector The single-tail detector can fail eas- ily if a small perturbation is added with watermarked images. The double-tail detector (Jiang, Zhang, and Gong 2023) is useful to detect perturbed images. The watermark decoded from original images has bitwise accuracy 0.5, while the watermark decoded from watermarked images has bitwise ac- curacy close to 1. So, if the bitwise accuracy of the watermark decoded from an image is much below than 0.5, the image is considered as a perturbed image. The FPR is calculated as follows: FPR(τ) = P(BA(w, ˆw) > τ) or P(BW(w, ˆw) < 1 − τ) = P(m > nτ) + P(m < n − nτ) = n X i=⌈nτ⌉   n i ! \\x121 2 \\x13n + ⌊n−nτ⌋ X i=1   n i ! \\x121 2 \\x13n (2) Visual Paraphrasing In the realm of AI-generated image detection, visual para- phrasing is a crucial method for confirming the authenticity'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "random.sample(pages_and_texts, k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3047</td>\n",
       "      <td>458</td>\n",
       "      <td>24</td>\n",
       "      <td>761.75</td>\n",
       "      <td>Image Watermarking Techniques are Brittle: Inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5037</td>\n",
       "      <td>714</td>\n",
       "      <td>33</td>\n",
       "      <td>1259.25</td>\n",
       "      <td>than ever. The findings of the latest (seventh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5331</td>\n",
       "      <td>794</td>\n",
       "      <td>40</td>\n",
       "      <td>1332.75</td>\n",
       "      <td>demonstrating the vulnerability of existing wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4985</td>\n",
       "      <td>806</td>\n",
       "      <td>41</td>\n",
       "      <td>1246.25</td>\n",
       "      <td>Generated Image Watermarked Image Difference W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2580</td>\n",
       "      <td>415</td>\n",
       "      <td>22</td>\n",
       "      <td>645.00</td>\n",
       "      <td>Tree Ring Stable Signature Figure 5: This figu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            0             3047              458                       24   \n",
       "1            1             5037              714                       33   \n",
       "2            2             5331              794                       40   \n",
       "3            3             4985              806                       41   \n",
       "4            4             2580              415                       22   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0            761.75  Image Watermarking Techniques are Brittle: Inv...  \n",
       "1           1259.25  than ever. The findings of the latest (seventh...  \n",
       "2           1332.75  demonstrating the vulnerability of existing wa...  \n",
       "3           1246.25  Generated Image Watermarked Image Difference W...  \n",
       "4            645.00  Tree Ring Stable Signature Figure 5: This figu...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.50</td>\n",
       "      <td>2713.14</td>\n",
       "      <td>414.79</td>\n",
       "      <td>26.21</td>\n",
       "      <td>678.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.18</td>\n",
       "      <td>2012.90</td>\n",
       "      <td>282.15</td>\n",
       "      <td>25.73</td>\n",
       "      <td>503.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>168.00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>42.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.25</td>\n",
       "      <td>730.75</td>\n",
       "      <td>162.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>182.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.50</td>\n",
       "      <td>2640.00</td>\n",
       "      <td>410.50</td>\n",
       "      <td>23.00</td>\n",
       "      <td>660.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.75</td>\n",
       "      <td>4741.50</td>\n",
       "      <td>680.50</td>\n",
       "      <td>38.25</td>\n",
       "      <td>1185.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.00</td>\n",
       "      <td>5741.00</td>\n",
       "      <td>806.00</td>\n",
       "      <td>96.00</td>\n",
       "      <td>1435.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count        14.00            14.00            14.00                    14.00   \n",
       "mean          6.50          2713.14           414.79                    26.21   \n",
       "std           4.18          2012.90           282.15                    25.73   \n",
       "min           0.00           168.00            46.00                     1.00   \n",
       "25%           3.25           730.75           162.50                     5.50   \n",
       "50%           6.50          2640.00           410.50                    23.00   \n",
       "75%           9.75          4741.50           680.50                    38.25   \n",
       "max          13.00          5741.00           806.00                    96.00   \n",
       "\n",
       "       page_token_count  \n",
       "count             14.00  \n",
       "mean             678.29  \n",
       "std              503.23  \n",
       "min               42.00  \n",
       "25%              182.69  \n",
       "50%              660.00  \n",
       "75%             1185.38  \n",
       "max             1435.25  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/ashhar21137/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4489a39525f45b78c75519ba9ea8dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78.49691772460938, 17.042869567871094], [14.924493789672852, 75.37962341308594]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True)\n",
    "# In case you want to reduce the maximum length:\n",
    "model.max_seq_length = 8192\n",
    "\n",
    "queries = [\n",
    "    \"how much protein should a female eat\",\n",
    "    \"summit define\",\n",
    "]\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n",
    "]\n",
    "\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "scores = (query_embeddings @ document_embeddings.T) * 100\n",
    "print(scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from torch import Tensor\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# def last_token_pool(last_hidden_states: Tensor,\n",
    "#                  attention_mask: Tensor) -> Tensor:\n",
    "#     left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "#     if left_padding:\n",
    "#         return last_hidden_states[:, -1]\n",
    "#     else:\n",
    "#         sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "#         batch_size = last_hidden_states.shape[0]\n",
    "#         return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "# def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "#     return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "\n",
    "# # Each query must come with a one-sentence instruction that describes the task\n",
    "# task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "# queries = [\n",
    "#     get_detailed_instruct(task, 'how much protein should a female eat'),\n",
    "#     get_detailed_instruct(task, 'summit define')\n",
    "# ]\n",
    "# # No need to add instruction for retrieval documents\n",
    "# documents = [\n",
    "#     \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "#     \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n",
    "# ]\n",
    "# input_texts = queries + documents\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-Qwen2-1.5B-instruct', trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained('Alibaba-NLP/gte-Qwen2-1.5B-instruct', trust_remote_code=True)\n",
    "\n",
    "# max_length = 8192\n",
    "\n",
    "# # Tokenize the input texts\n",
    "# batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt')\n",
    "# outputs = model(**batch_dict)\n",
    "# embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "# # normalize embeddings\n",
    "# embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "# scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "# print(scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "doc = nlp(\"Hi, I saw you standing there. What were you doing?\")\n",
    "\n",
    "# assert(len(list(doc.sents))) == 3\n",
    "\n",
    "s = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi, I saw you standing there.', 'What were you doing?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "txt = \"Hi, I saw you standing there. What were you doing?\"\n",
    "l = nltk.tokenize.sent_tokenize(txt, language='english')\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 6,\n",
       " 'page_char_count': 2700,\n",
       " 'page_word_count': 406,\n",
       " 'page_sentence_count_raw': 16,\n",
       " 'page_token_count': 675.0,\n",
       " 'text': 'Watermarking Method Watermark Detection Rate (η) Pre Attack Post Attack Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase COCO (Lin et al. 2015) DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - Table 1: Watermark Detection Rates (η) for Different Techniques and Attacks delicate equilibrium can be achieved by controlling the hy- perparameters of the image-to-image diffusion model. (a) Watermarked (b) Visual Paraphrase (c) Watermarked (d) Visual Paraphrase Figure 9: Comparison of watermarked images (a, c) with their visual paraphrased counterparts (b, d). Each colored box (red, yellow, and blue) represents a one-to-one comparison of the same region in the original and paraphrased images, highlighting how visual paraphrasing alters specific elements within these regions, resulting in information loss. One crucial hyperparameter is the strength parameter, which determines the extent of transformation applied to the reference image. A higher strength value results in more significant noise being added to the image, thereby deviating further from the original. Conversely, a lower strength value preserves the image’s fidelity to the original, reducing the risk of information loss. By carefully adjusting the strength parameter, researchers can regulate the amount of noise in- troduced during the diffusion process, thereby mitigating potential information loss while maintaining image quality. Another vital hyperparameter is the guidance scale, which influences the model’s adherence to the text prompt during image generation. A higher guidance scale encourages the model to prioritize fidelity to the prompt, potentially sacrific- ing image quality in the process. Conversely, a lower guid- ance scale allows for more flexibility in image generation, potentially leading to higher-quality results at the expense of strict adherence to the text prompt. By fine-tuning the guid- ance scale, researchers can strike an optimal balance between textual fidelity and image quality, thereby mitigating infor- mation loss while ensuring the generated image’s coherence with the provided prompt. Human evaluation on Visual Paraphrase Figure 10: Heatmap of MOS scores with 500 assessed sam- ples for each category. Conclusion In conclusion, the imperative for robust AI-generated image detection mechanisms has never been more pressing. The proliferation of highly realistic generative models poses sig- nificant challenges to content ownership and the dissemina- tion of misinformation. By exploring watermarking methods, visual paraphrasing scenarios, and the control of information'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_texts[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 846.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts) : \n",
    "    text = item['text']\n",
    "    item[\"sentences\"] = nltk.tokenize.sent_tokenize(text, language='english') \n",
    "\n",
    "    item['page_sentence_count_nltk'] = len(item['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 9,\n",
       " 'page_char_count': 1715,\n",
       " 'page_word_count': 250,\n",
       " 'page_sentence_count_raw': 49,\n",
       " 'page_token_count': 428.75,\n",
       " 'text': 'Lopes, R. G.; Salimans, T.; Ho, J.; Fleet, D. J.; and Norouzi, M. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. arXiv:2205.11487. Song, J.; Meng, C.; and Ermon, S. 2022. Denoising Diffusion Implicit Models. arXiv:2010.02502. T.J. Thomson, P. D., Daniel Angus. 2020. 3.2 billion images and 720,000 hours of video are shared online daily. Can you sort real from fake? Wen, B.; and Aydore, S. 2019. ROMark: A Ro- bust Watermarking System Using Adversarial Training. arXiv:1910.01221. Wen, Y.; Kirchenbauer, J.; Geiping, J.; and Goldstein, T. 2023. Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust. arXiv:2305.20030. White-House. 2023. Blueprint for an AI Bill of Rights: Mak- ing Automated Systems Work For the American People. Yu, N.; Skripniuk, V.; Abdelnabi, S.; and Fritz, M. 2021. Arti- ficial fingerprinting for generative models: Rooting deepfake attribution in training data. In Proceedings of the IEEE/CVF International conference on computer vision, 14448–14457. Yuan, Z.; Liu, D.; Zhang, X.; and Su, Q. 2020. New im- age blind watermarking method based on two-dimensional discrete cosine transform. Optik, 204: 164152. Zhang, C.; Liao, Q.; Rakhlin, A.; Miranda, B.; Golowich, N.; and Poggio, T. 2018. Theory of Deep Learning IIb: Optimization Properties of SGD. arXiv:1801.02254. Zhang, L.; Liu, X.; Martin, A. V.; Bearfield, C. X.; Brun, Y.; and Guan, H. 2024. Robust Image Watermarking using Stable Diffusion. arXiv preprint arXiv:2401.04247. Zhao, X.; Zhang, K.; Wang, Y.-X.; and Li, L. 2023. Genera- tive Autoencoders as Watermark Attackers: Analyses of Vul- nerabilities and Threats. arXiv preprint arXiv:2306.01953. Appendix',\n",
       " 'sentences': ['Lopes, R. G.; Salimans, T.; Ho, J.; Fleet, D. J.; and Norouzi, M. 2022.',\n",
       "  'Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.',\n",
       "  'arXiv:2205.11487.',\n",
       "  'Song, J.; Meng, C.; and Ermon, S. 2022.',\n",
       "  'Denoising Diffusion Implicit Models.',\n",
       "  'arXiv:2010.02502.',\n",
       "  'T.J. Thomson, P. D., Daniel Angus.',\n",
       "  '2020.',\n",
       "  '3.2 billion images and 720,000 hours of video are shared online daily.',\n",
       "  'Can you sort real from fake?',\n",
       "  'Wen, B.; and Aydore, S. 2019.',\n",
       "  'ROMark: A Ro- bust Watermarking System Using Adversarial Training.',\n",
       "  'arXiv:1910.01221.',\n",
       "  'Wen, Y.; Kirchenbauer, J.; Geiping, J.; and Goldstein, T. 2023.',\n",
       "  'Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust.',\n",
       "  'arXiv:2305.20030.',\n",
       "  'White-House.',\n",
       "  '2023.',\n",
       "  'Blueprint for an AI Bill of Rights: Mak- ing Automated Systems Work For the American People.',\n",
       "  'Yu, N.; Skripniuk, V.; Abdelnabi, S.; and Fritz, M. 2021.',\n",
       "  'Arti- ficial fingerprinting for generative models: Rooting deepfake attribution in training data.',\n",
       "  'In Proceedings of the IEEE/CVF International conference on computer vision, 14448–14457.',\n",
       "  'Yuan, Z.; Liu, D.; Zhang, X.; and Su, Q.',\n",
       "  '2020.',\n",
       "  'New im- age blind watermarking method based on two-dimensional discrete cosine transform.',\n",
       "  'Optik, 204: 164152.',\n",
       "  'Zhang, C.; Liao, Q.; Rakhlin, A.; Miranda, B.; Golowich, N.; and Poggio, T. 2018.',\n",
       "  'Theory of Deep Learning IIb: Optimization Properties of SGD.',\n",
       "  'arXiv:1801.02254.',\n",
       "  'Zhang, L.; Liu, X.; Martin, A. V.; Bearfield, C. X.; Brun, Y.; and Guan, H. 2024.',\n",
       "  'Robust Image Watermarking using Stable Diffusion.',\n",
       "  'arXiv preprint arXiv:2401.04247.',\n",
       "  'Zhao, X.; Zhang, K.; Wang, Y.-X.',\n",
       "  '; and Li, L. 2023.',\n",
       "  'Genera- tive Autoencoders as Watermark Attackers: Analyses of Vul- nerabilities and Threats.',\n",
       "  'arXiv preprint arXiv:2306.01953.',\n",
       "  'Appendix'],\n",
       " 'page_sentence_count_nltk': 37}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_texts[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>page_sentence_count_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3047</td>\n",
       "      <td>458</td>\n",
       "      <td>24</td>\n",
       "      <td>761.75</td>\n",
       "      <td>Image Watermarking Techniques are Brittle: Inv...</td>\n",
       "      <td>[Image Watermarking Techniques are Brittle: In...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5037</td>\n",
       "      <td>714</td>\n",
       "      <td>33</td>\n",
       "      <td>1259.25</td>\n",
       "      <td>than ever. The findings of the latest (seventh...</td>\n",
       "      <td>[than ever., The findings of the latest (seven...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5331</td>\n",
       "      <td>794</td>\n",
       "      <td>40</td>\n",
       "      <td>1332.75</td>\n",
       "      <td>demonstrating the vulnerability of existing wa...</td>\n",
       "      <td>[demonstrating the vulnerability of existing w...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4985</td>\n",
       "      <td>806</td>\n",
       "      <td>41</td>\n",
       "      <td>1246.25</td>\n",
       "      <td>Generated Image Watermarked Image Difference W...</td>\n",
       "      <td>[Generated Image Watermarked Image Difference ...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2580</td>\n",
       "      <td>415</td>\n",
       "      <td>22</td>\n",
       "      <td>645.00</td>\n",
       "      <td>Tree Ring Stable Signature Figure 5: This figu...</td>\n",
       "      <td>[Tree Ring Stable Signature Figure 5: This fig...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            0             3047              458                       24   \n",
       "1            1             5037              714                       33   \n",
       "2            2             5331              794                       40   \n",
       "3            3             4985              806                       41   \n",
       "4            4             2580              415                       22   \n",
       "\n",
       "   page_token_count                                               text  \\\n",
       "0            761.75  Image Watermarking Techniques are Brittle: Inv...   \n",
       "1           1259.25  than ever. The findings of the latest (seventh...   \n",
       "2           1332.75  demonstrating the vulnerability of existing wa...   \n",
       "3           1246.25  Generated Image Watermarked Image Difference W...   \n",
       "4            645.00  Tree Ring Stable Signature Figure 5: This figu...   \n",
       "\n",
       "                                           sentences  page_sentence_count_nltk  \n",
       "0  [Image Watermarking Techniques are Brittle: In...                        23  \n",
       "1  [than ever., The findings of the latest (seven...                        33  \n",
       "2  [demonstrating the vulnerability of existing w...                        40  \n",
       "3  [Generated Image Watermarked Image Difference ...                        42  \n",
       "4  [Tree Ring Stable Signature Figure 5: This fig...                        23  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.50</td>\n",
       "      <td>2713.14</td>\n",
       "      <td>414.79</td>\n",
       "      <td>26.21</td>\n",
       "      <td>678.29</td>\n",
       "      <td>23.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.18</td>\n",
       "      <td>2012.90</td>\n",
       "      <td>282.15</td>\n",
       "      <td>25.73</td>\n",
       "      <td>503.23</td>\n",
       "      <td>20.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>168.00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.25</td>\n",
       "      <td>730.75</td>\n",
       "      <td>162.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>182.69</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.50</td>\n",
       "      <td>2640.00</td>\n",
       "      <td>410.50</td>\n",
       "      <td>23.00</td>\n",
       "      <td>660.00</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.75</td>\n",
       "      <td>4741.50</td>\n",
       "      <td>680.50</td>\n",
       "      <td>38.25</td>\n",
       "      <td>1185.38</td>\n",
       "      <td>36.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.00</td>\n",
       "      <td>5741.00</td>\n",
       "      <td>806.00</td>\n",
       "      <td>96.00</td>\n",
       "      <td>1435.25</td>\n",
       "      <td>73.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count        14.00            14.00            14.00                    14.00   \n",
       "mean          6.50          2713.14           414.79                    26.21   \n",
       "std           4.18          2012.90           282.15                    25.73   \n",
       "min           0.00           168.00            46.00                     1.00   \n",
       "25%           3.25           730.75           162.50                     5.50   \n",
       "50%           6.50          2640.00           410.50                    23.00   \n",
       "75%           9.75          4741.50           680.50                    38.25   \n",
       "max          13.00          5741.00           806.00                    96.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_nltk  \n",
       "count             14.00                     14.00  \n",
       "mean             678.29                     23.79  \n",
       "std              503.23                     20.51  \n",
       "min               42.00                      1.00  \n",
       "25%              182.69                      5.50  \n",
       "50%              660.00                     23.00  \n",
       "75%             1185.38                     36.00  \n",
       "max             1435.25                     73.00  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We will chunk sentences into groups of 5 '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We will chunk sentences into groups of 5 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [20]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 10\n",
    "\n",
    "def chunking(input_list , chunk_size) :\n",
    "    l = [input_list[i : i+ chunk_size] for i in range(0,len(input_list), chunk_size)]\n",
    "    return l \n",
    "\n",
    "test = list(range(21))\n",
    "\n",
    "chunking(test,chunk_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 84006.09it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts) : \n",
    "    item[\"chunks\"] = chunking(item['sentences'], chunk_size)\n",
    "    item['num_chunks'] = len(item[\"chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Watermarking Method Watermark Detection Rate (η) Pre Attack Post Attack Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase COCO (Lin et al.',\n",
       "  '2015) DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - Table 1: Watermark Detection Rates (η) for Different Techniques and Attacks delicate equilibrium can be achieved by controlling the hy- perparameters of the image-to-image diffusion model.',\n",
       "  '(a) Watermarked (b) Visual Paraphrase (c) Watermarked (d) Visual Paraphrase Figure 9: Comparison of watermarked images (a, c) with their visual paraphrased counterparts (b, d).',\n",
       "  'Each colored box (red, yellow, and blue) represents a one-to-one comparison of the same region in the original and paraphrased images, highlighting how visual paraphrasing alters specific elements within these regions, resulting in information loss.',\n",
       "  'One crucial hyperparameter is the strength parameter, which determines the extent of transformation applied to the reference image.',\n",
       "  'A higher strength value results in more significant noise being added to the image, thereby deviating further from the original.',\n",
       "  'Conversely, a lower strength value preserves the image’s fidelity to the original, reducing the risk of information loss.',\n",
       "  'By carefully adjusting the strength parameter, researchers can regulate the amount of noise in- troduced during the diffusion process, thereby mitigating potential information loss while maintaining image quality.',\n",
       "  'Another vital hyperparameter is the guidance scale, which influences the model’s adherence to the text prompt during image generation.',\n",
       "  'A higher guidance scale encourages the model to prioritize fidelity to the prompt, potentially sacrific- ing image quality in the process.'],\n",
       " ['Conversely, a lower guid- ance scale allows for more flexibility in image generation, potentially leading to higher-quality results at the expense of strict adherence to the text prompt.',\n",
       "  'By fine-tuning the guid- ance scale, researchers can strike an optimal balance between textual fidelity and image quality, thereby mitigating infor- mation loss while ensuring the generated image’s coherence with the provided prompt.',\n",
       "  'Human evaluation on Visual Paraphrase Figure 10: Heatmap of MOS scores with 500 assessed sam- ples for each category.',\n",
       "  'Conclusion In conclusion, the imperative for robust AI-generated image detection mechanisms has never been more pressing.',\n",
       "  'The proliferation of highly realistic generative models poses sig- nificant challenges to content ownership and the dissemina- tion of misinformation.',\n",
       "  'By exploring watermarking methods, visual paraphrasing scenarios, and the control of information']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_texts[6]['chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_texts[6]['num_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>page_sentence_count_nltk</th>\n",
       "      <th>chunks</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3047</td>\n",
       "      <td>458</td>\n",
       "      <td>24</td>\n",
       "      <td>761.75</td>\n",
       "      <td>Image Watermarking Techniques are Brittle: Inv...</td>\n",
       "      <td>[Image Watermarking Techniques are Brittle: In...</td>\n",
       "      <td>23</td>\n",
       "      <td>[[Image Watermarking Techniques are Brittle: I...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5037</td>\n",
       "      <td>714</td>\n",
       "      <td>33</td>\n",
       "      <td>1259.25</td>\n",
       "      <td>than ever. The findings of the latest (seventh...</td>\n",
       "      <td>[than ever., The findings of the latest (seven...</td>\n",
       "      <td>33</td>\n",
       "      <td>[[than ever., The findings of the latest (seve...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5331</td>\n",
       "      <td>794</td>\n",
       "      <td>40</td>\n",
       "      <td>1332.75</td>\n",
       "      <td>demonstrating the vulnerability of existing wa...</td>\n",
       "      <td>[demonstrating the vulnerability of existing w...</td>\n",
       "      <td>40</td>\n",
       "      <td>[[demonstrating the vulnerability of existing ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4985</td>\n",
       "      <td>806</td>\n",
       "      <td>41</td>\n",
       "      <td>1246.25</td>\n",
       "      <td>Generated Image Watermarked Image Difference W...</td>\n",
       "      <td>[Generated Image Watermarked Image Difference ...</td>\n",
       "      <td>42</td>\n",
       "      <td>[[Generated Image Watermarked Image Difference...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2580</td>\n",
       "      <td>415</td>\n",
       "      <td>22</td>\n",
       "      <td>645.00</td>\n",
       "      <td>Tree Ring Stable Signature Figure 5: This figu...</td>\n",
       "      <td>[Tree Ring Stable Signature Figure 5: This fig...</td>\n",
       "      <td>23</td>\n",
       "      <td>[[Tree Ring Stable Signature Figure 5: This fi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            0             3047              458                       24   \n",
       "1            1             5037              714                       33   \n",
       "2            2             5331              794                       40   \n",
       "3            3             4985              806                       41   \n",
       "4            4             2580              415                       22   \n",
       "\n",
       "   page_token_count                                               text  \\\n",
       "0            761.75  Image Watermarking Techniques are Brittle: Inv...   \n",
       "1           1259.25  than ever. The findings of the latest (seventh...   \n",
       "2           1332.75  demonstrating the vulnerability of existing wa...   \n",
       "3           1246.25  Generated Image Watermarked Image Difference W...   \n",
       "4            645.00  Tree Ring Stable Signature Figure 5: This figu...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [Image Watermarking Techniques are Brittle: In...   \n",
       "1  [than ever., The findings of the latest (seven...   \n",
       "2  [demonstrating the vulnerability of existing w...   \n",
       "3  [Generated Image Watermarked Image Difference ...   \n",
       "4  [Tree Ring Stable Signature Figure 5: This fig...   \n",
       "\n",
       "   page_sentence_count_nltk  \\\n",
       "0                        23   \n",
       "1                        33   \n",
       "2                        40   \n",
       "3                        42   \n",
       "4                        23   \n",
       "\n",
       "                                              chunks  num_chunks  \n",
       "0  [[Image Watermarking Techniques are Brittle: I...           3  \n",
       "1  [[than ever., The findings of the latest (seve...           4  \n",
       "2  [[demonstrating the vulnerability of existing ...           4  \n",
       "3  [[Generated Image Watermarked Image Difference...           5  \n",
       "4  [[Tree Ring Stable Signature Figure 5: This fi...           3  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_nltk</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.50</td>\n",
       "      <td>2713.14</td>\n",
       "      <td>414.79</td>\n",
       "      <td>26.21</td>\n",
       "      <td>678.29</td>\n",
       "      <td>23.79</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.18</td>\n",
       "      <td>2012.90</td>\n",
       "      <td>282.15</td>\n",
       "      <td>25.73</td>\n",
       "      <td>503.23</td>\n",
       "      <td>20.51</td>\n",
       "      <td>2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>168.00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.25</td>\n",
       "      <td>730.75</td>\n",
       "      <td>162.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>182.69</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.50</td>\n",
       "      <td>2640.00</td>\n",
       "      <td>410.50</td>\n",
       "      <td>23.00</td>\n",
       "      <td>660.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.75</td>\n",
       "      <td>4741.50</td>\n",
       "      <td>680.50</td>\n",
       "      <td>38.25</td>\n",
       "      <td>1185.38</td>\n",
       "      <td>36.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.00</td>\n",
       "      <td>5741.00</td>\n",
       "      <td>806.00</td>\n",
       "      <td>96.00</td>\n",
       "      <td>1435.25</td>\n",
       "      <td>73.00</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count        14.00            14.00            14.00                    14.00   \n",
       "mean          6.50          2713.14           414.79                    26.21   \n",
       "std           4.18          2012.90           282.15                    25.73   \n",
       "min           0.00           168.00            46.00                     1.00   \n",
       "25%           3.25           730.75           162.50                     5.50   \n",
       "50%           6.50          2640.00           410.50                    23.00   \n",
       "75%           9.75          4741.50           680.50                    38.25   \n",
       "max          13.00          5741.00           806.00                    96.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_nltk  num_chunks  \n",
       "count             14.00                     14.00       14.00  \n",
       "mean             678.29                     23.79        2.93  \n",
       "std              503.23                     20.51        2.02  \n",
       "min               42.00                      1.00        1.00  \n",
       "25%              182.69                      5.50        1.00  \n",
       "50%              660.00                     23.00        3.00  \n",
       "75%             1185.38                     36.00        4.00  \n",
       "max             1435.25                     73.00        8.00  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 6733.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for chunk in item[\"chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"chunks\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Watermarking Method Watermark Detection Rate (η) Pre Attack Post Attack Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase COCO (Lin et al.',\n",
       "  '2015) DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - Table 1: Watermark Detection Rates (η) for Different Techniques and Attacks delicate equilibrium can be achieved by controlling the hy- perparameters of the image-to-image diffusion model.',\n",
       "  '(a) Watermarked (b) Visual Paraphrase (c) Watermarked (d) Visual Paraphrase Figure 9: Comparison of watermarked images (a, c) with their visual paraphrased counterparts (b, d).',\n",
       "  'Each colored box (red, yellow, and blue) represents a one-to-one comparison of the same region in the original and paraphrased images, highlighting how visual paraphrasing alters specific elements within these regions, resulting in information loss.',\n",
       "  'One crucial hyperparameter is the strength parameter, which determines the extent of transformation applied to the reference image.',\n",
       "  'A higher strength value results in more significant noise being added to the image, thereby deviating further from the original.',\n",
       "  'Conversely, a lower strength value preserves the image’s fidelity to the original, reducing the risk of information loss.',\n",
       "  'By carefully adjusting the strength parameter, researchers can regulate the amount of noise in- troduced during the diffusion process, thereby mitigating potential information loss while maintaining image quality.',\n",
       "  'Another vital hyperparameter is the guidance scale, which influences the model’s adherence to the text prompt during image generation.',\n",
       "  'A higher guidance scale encourages the model to prioritize fidelity to the prompt, potentially sacrific- ing image quality in the process.'],\n",
       " ['Conversely, a lower guid- ance scale allows for more flexibility in image generation, potentially leading to higher-quality results at the expense of strict adherence to the text prompt.',\n",
       "  'By fine-tuning the guid- ance scale, researchers can strike an optimal balance between textual fidelity and image quality, thereby mitigating infor- mation loss while ensuring the generated image’s coherence with the provided prompt.',\n",
       "  'Human evaluation on Visual Paraphrase Figure 10: Heatmap of MOS scores with 500 assessed sam- ples for each category.',\n",
       "  'Conclusion In conclusion, the imperative for robust AI-generated image detection mechanisms has never been more pressing.',\n",
       "  'The proliferation of highly realistic generative models poses sig- nificant challenges to content ownership and the dissemina- tion of misinformation.',\n",
       "  'By exploring watermarking methods, visual paraphrasing scenarios, and the control of information']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_texts[6]['chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 1,\n",
       " 'chunks': 'Visual paraphrasing is not yet a widely recognized sub-discipline. However, this paper demonstrates how visual paraphrasing can be accomplished using state-of-the-art text-to-image generation systems. This paper presents a critical assessment, empirically',\n",
       " 'chunk_char_count': 255,\n",
       " 'chunk_word_count': 31,\n",
       " 'chunk_token_count': 63.75}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_texts[6]['num_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 13,\n",
       " 'chunks': 'Watermarked Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase (Ours) η = 1 η = 0.989 η = 0.841 η = 0.624 η = 0.671 η = 0.263 η = 1 η = 0.991 η = 0.813 η = 0.611 η = 0.633 η = 0.334 η = 1 η = 0.984 η = 0.837 η = 0.656 η = 0.603 η = 0.297 η = 1 η = 0.994 η = 0.784 η = 0.609 η = 0.579 η = 0.273 η = 1 η = 0.997 η = 0.759 η = 0.702 η = 0.682 η = 0.311 Table 4: Placeholder, these are stable signature examples.need to put tree ring, ZoDiac, Gaussian Shading, dwtdctsvd, HiDDen examples. The figure shows watermarked images, images under various attacks, and our visual paraphrase method. The attacks include Brightness adjustment, Rotation, JPEG Compression, and Gaussian Noise, along with our Visual Paraphrase method.η comparisons, representing watermark detection score (bit accuracy), are also provided.',\n",
       " 'chunk_char_count': 824,\n",
       " 'chunk_word_count': 159,\n",
       " 'chunk_token_count': 206.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>41.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.34</td>\n",
       "      <td>924.10</td>\n",
       "      <td>139.95</td>\n",
       "      <td>231.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.57</td>\n",
       "      <td>527.92</td>\n",
       "      <td>76.22</td>\n",
       "      <td>131.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>168.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>42.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.00</td>\n",
       "      <td>481.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>120.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.00</td>\n",
       "      <td>908.00</td>\n",
       "      <td>155.00</td>\n",
       "      <td>227.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.00</td>\n",
       "      <td>1309.00</td>\n",
       "      <td>194.00</td>\n",
       "      <td>327.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.00</td>\n",
       "      <td>2223.00</td>\n",
       "      <td>295.00</td>\n",
       "      <td>555.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count        41.00             41.00             41.00              41.00\n",
       "mean          5.34            924.10            139.95             231.02\n",
       "std           3.57            527.92             76.22             131.98\n",
       "min           0.00            168.00             31.00              42.00\n",
       "25%           2.00            481.00             65.00             120.25\n",
       "50%           5.00            908.00            155.00             227.00\n",
       "75%           8.00           1309.00            194.00             327.25\n",
       "max          13.00           2223.00            295.00             555.75"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 3,\n",
       "  'chunks': '\\x121 2 \\x13n + ⌊n−nτ⌋ X i=1  n i !\\x121 2 \\x13n (2) Visual Paraphrasing In the realm of AI-generated image detection, visual para- phrasing is a crucial method for confirming the authenticity',\n",
       "  'chunk_char_count': 180,\n",
       "  'chunk_word_count': 34,\n",
       "  'chunk_token_count': 45.0}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks, k = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering very short chunks, they may not contain much info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removing very short chunks '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Removing very short chunks \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: chunks, dtype: object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"chunk_token_count\"] <= min_token_len][\"chunks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_token_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_token_len\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39miterrows(): \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCHunk token count : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_token_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | text : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/pandas/core/generic.py:6115\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[0;32m-> 6115\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6116\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[0;32m~/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/pandas/core/sample.py:152\u001b[0m, in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    154\u001b[0m )\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:945\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "for row in df[df[\"chunk_token_count\"] <= min_token_len].sample(2).iterrows(): \n",
    "    print(f'CHunk token count : {row[1][\"chunk_token_count\"]} | text : {row[1][\"chunks\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 0,\n",
       "  'chunks': 'Image Watermarking Techniques are Brittle: Investigating Visual Paraphrasing for De-Watermarking AI-Generated Images Anonymous submission Forward Diffusion Process Text Caption Generator Black Box An image of Pope Francis smiling and wearing a white jacket, surrounded by a crowd White Box An image of Pope Francis smiling and wearing a white jacket, surrounded by a crowd E UNet .... D UNet Denoising Dewatermarked Image Figure 1: Block diagram of the visual paraphrasing technique illustrating the dewatermarking process. The diagram includes a forward diffusion process for encoding and decoding images to generate visually paraphrased outputs. It features a White Box scenario, where access to prompts is available, allowing direct manipulation of the image using descriptive prompts. In contrast, the Black Box scenario does not have access to prompts, relying on a caption generator (Kosmos 2)(Peng et al.2023) to interpret and paraphrase the image context indirectly. Abstract With the rise of potent text-to-image generation systems such as Stable Diffusion (Rombach et al.',\n",
       "  'chunk_char_count': 1081,\n",
       "  'chunk_word_count': 158,\n",
       "  'chunk_token_count': 270.25},\n",
       " {'page_number': 0,\n",
       "  'chunks': '2022a), Midjourney (Holz 2022), Imagen (Saharia et al.2022), and DALL-E (Ramesh et al.2021), the potential for misuse of these tools has esca- lated. In response to the need to curb the circulation of po- tentially misleading visuals created by AI-generated content, companies like Meta have intensified their efforts in imple- menting watermarking techniques on AI-generated images. However, in this paper, we contend that all existing image watermarking methods are fragile and susceptible to being circumvented through visual paraphrasing techniques. We ex- plore two potential scenarios: (i) when we possess the original prompt used to generate an AI-generated image, and (ii) when encountering an image in the wild, potentially watermarked. We empirically assert that in both scenarios, visual paraphrase techniques can effectively remove watermarks from these im- ages. This paper solely critiques current techniques and em- pirically demonstrates the shortcomings of state-of-the-art watermarking techniques for images. AI Generated Image Detection - the Necessity The extraordinary benefits of very large generative AI models such as Stable Diffusion(s), DALL-E(s), Midjourney, Ima- gen, GPT(s), and SORA (to be released) also come with a substantial risk of misuse. The alarm is reflected in the open letter by thousands of researchers and tech leaders in March 2023 for a six-month moratorium on the training of AI sys- tems that are more sophisticated than GPT-4.',\n",
       "  'chunk_char_count': 1474,\n",
       "  'chunk_word_count': 218,\n",
       "  'chunk_token_count': 368.5}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks_over_threshold = df[df[\"chunk_token_count\"] > min_token_len].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_threshold[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 8,\n",
       "  'chunks': 'High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), 10684–10695. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E.; Ghasemipour, S. K. S.; Ayan, B. K.; Mahdavi, S. S.;',\n",
       "  'chunk_char_count': 287,\n",
       "  'chunk_word_count': 44,\n",
       "  'chunk_token_count': 71.75},\n",
       " {'page_number': 7,\n",
       "  'chunks': 'Watermarked Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase (Ours) η = 1 η = 0.989 η = 0.841 η = 0.624 η = 0.671 η = 0.263 η = 1 η = 0.991 η = 0.813 η = 0.611 η = 0.633 η = 0.334 η = 1 η = 0.984 η = 0.837 η = 0.656 η = 0.603 η = 0.297 η = 1 η = 0.994 η = 0.784 η = 0.609 η = 0.579 η = 0.273 η = 1 η = 0.997 η = 0.759 η = 0.702 η = 0.682 η = 0.311 Table 2: The figure shows watermarked images, images under various attacks, and our visual paraphrase method. The attacks include Brightness adjustment, Rotation, JPEG Compression, and Gaussian Noise, along with our Visual Paraphrase method.η comparisons, representing watermark detection score (bit accuracy), are also provided.',\n",
       "  'chunk_char_count': 698,\n",
       "  'chunk_word_count': 143,\n",
       "  'chunk_token_count': 174.5}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks_over_threshold, k =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nfrom sentence_transformers import SentenceTransformer\\n\\nmodel = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True)\\n# In case you want to reduce the maximum length:\\nmodel.max_seq_length = 8192\\n\\nqueries = [\\n    \"how much protein should a female eat\",\\n    \"summit define\",\\n]\\ndocuments = [\\n    \"As a general guideline, the CDC\\'s average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you\\'ll need to increase that if you\\'re expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\\n    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\\n]\\n\\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\\ndocument_embeddings = model.encode(documents)\\n\\nscores = (query_embeddings @ document_embeddings.T) * 100\\nprint(scores.tolist())\\n\\n\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True)\n",
    "# In case you want to reduce the maximum length:\n",
    "model.max_seq_length = 8192\n",
    "\n",
    "queries = [\n",
    "    \"how much protein should a female eat\",\n",
    "    \"summit define\",\n",
    "]\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n",
    "]\n",
    "\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "scores = (query_embeddings @ document_embeddings.T) * 100\n",
    "print(scores.tolist())\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing \"\"\"\n",
    "\n",
    "test_sentences = [\"Testing a local rag system.\", \" I hope this works.\", \"I am wasting too much time on this.\"]\n",
    "\n",
    "test_embeddings = model.encode(test_sentences)\n",
    "embeddings_dict = dict(zip(test_sentences, test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Testing a local rag system.\n",
      "Embedding: [[ 1.8763652e-02  2.7032044e-02  2.6048908e-02 ...  1.8244106e-02\n",
      "  -4.5662776e-02 -2.0056074e-02]\n",
      " [-5.5436410e-02  5.1500294e-02  2.7566036e-02 ... -2.5363853e-02\n",
      "  -1.6618188e-02 -3.0015926e-03]\n",
      " [-4.8850998e-02  1.0981756e-02  3.5240576e-02 ... -1.3871044e-02\n",
      "  -9.4725488e-05 -1.8300353e-02]]\n",
      "Embedding size : (3, 1536)\n",
      "\n",
      "Sentence:  I hope this works.\n",
      "Embedding: [[ 1.8763652e-02  2.7032044e-02  2.6048908e-02 ...  1.8244106e-02\n",
      "  -4.5662776e-02 -2.0056074e-02]\n",
      " [-5.5436410e-02  5.1500294e-02  2.7566036e-02 ... -2.5363853e-02\n",
      "  -1.6618188e-02 -3.0015926e-03]\n",
      " [-4.8850998e-02  1.0981756e-02  3.5240576e-02 ... -1.3871044e-02\n",
      "  -9.4725488e-05 -1.8300353e-02]]\n",
      "Embedding size : (3, 1536)\n",
      "\n",
      "Sentence: I am wasting too much time on this.\n",
      "Embedding: [[ 1.8763652e-02  2.7032044e-02  2.6048908e-02 ...  1.8244106e-02\n",
      "  -4.5662776e-02 -2.0056074e-02]\n",
      " [-5.5436410e-02  5.1500294e-02  2.7566036e-02 ... -2.5363853e-02\n",
      "  -1.6618188e-02 -3.0015926e-03]\n",
      " [-4.8850998e-02  1.0981756e-02  3.5240576e-02 ... -1.3871044e-02\n",
      "  -9.4725488e-05 -1.8300353e-02]]\n",
      "Embedding size : (3, 1536)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the embeddings\n",
    "for test_sentences, test_sentences in embeddings_dict.items():\n",
    "    print(\"Sentence:\", test_sentences)\n",
    "    print(\"Embedding:\", test_embeddings)\n",
    "    print(\"Embedding size :\", test_embeddings.shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Watermarking is executed by subtly altering the latent repre- sentation in a manner imperceptible to human observation but discernible by a pretrained watermark extractor network. The essence of the Stable Signature technique revolves around refining the LDM decoder to yield images that manifest a predetermined signature when scrutinized by the watermark extractor network. This involves the minimization of a loss function that amalgamates the reconstruction loss and the watermark loss, wherein the former assesses the variance be- tween the generated image and the target image, and the latter quantifies the discrepancy between the signature of the gener- ated image and the desired watermark signature. The balance between these two aspects is regulated by a hyperparameter denoted as λ. In essence, the Stable Signature approach operates as fol- lows: First, a watermark extractor network is trained to recog- nize a particular watermark within images. Subsequently, the LDM decoder is meticulously fine-tuned to produce images that display the predefined signature when passed through the watermark extractor network. To generate a watermarked image, one simply samples a latent representation from the LDM and decodes it employing the finely calibrated decoder. Standard Training We use SGD (Zhang et al.2018) to optimize the loss function: Pn i=1 li(D(Xw), w). Adversarial Training We can apply adversarial (Wen and Aydore 2019) training in order to improve robustness against post-processing.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = [item[\"chunks\"] for item in pages_and_chunks_over_threshold]\n",
    "text_chunks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:02<00:00, 16.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings one by one on the GPU\n",
    "for item in tqdm(pages_and_chunks_over_threshold):\n",
    "    item[\"embedding\"] = model.encode(item[\"chunks\"], batch_size=32, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunk_embeddings = model.encode(text_chunks,\n",
    "#                                                batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
    "#                                                convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
    "\n",
    "# text_chunk_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 0,\n",
       " 'chunks': 'Image Watermarking Techniques are Brittle: Investigating Visual Paraphrasing for De-Watermarking AI-Generated Images Anonymous submission Forward Diffusion Process Text Caption Generator Black Box An image of Pope Francis smiling and wearing a white jacket, surrounded by a crowd White Box An image of Pope Francis smiling and wearing a white jacket, surrounded by a crowd E UNet .... D UNet Denoising Dewatermarked Image Figure 1: Block diagram of the visual paraphrasing technique illustrating the dewatermarking process. The diagram includes a forward diffusion process for encoding and decoding images to generate visually paraphrased outputs. It features a White Box scenario, where access to prompts is available, allowing direct manipulation of the image using descriptive prompts. In contrast, the Black Box scenario does not have access to prompts, relying on a caption generator (Kosmos 2)(Peng et al.2023) to interpret and paraphrase the image context indirectly. Abstract With the rise of potent text-to-image generation systems such as Stable Diffusion (Rombach et al.',\n",
       " 'chunk_char_count': 1081,\n",
       " 'chunk_word_count': 158,\n",
       " 'chunk_token_count': 270.25,\n",
       " 'embedding': array([ 0.00183125,  0.04405575,  0.01164224, ...,  0.008254  ,\n",
       "         0.01509167, -0.01082556], dtype=float32)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks_over_threshold[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunk_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk_embeddings_df = pd.DataFrame(pages_and_chunks_over_threshold)\n",
    "embedding_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunk_embeddings_df.to_csv(embedding_df_save_path, index=False, escapechar='\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunks</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Image Watermarking Techniques are Brittle: Inv...</td>\n",
       "      <td>1081</td>\n",
       "      <td>158</td>\n",
       "      <td>270.25</td>\n",
       "      <td>[ 0.00183125  0.04405575  0.01164224 ...  0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022a), Midjourney (Holz 2022), Imagen (Sahari...</td>\n",
       "      <td>1474</td>\n",
       "      <td>218</td>\n",
       "      <td>368.50</td>\n",
       "      <td>[ 0.02201897  0.04330228  0.01203674 ...  0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The central concern noted in the letter (Marcu...</td>\n",
       "      <td>484</td>\n",
       "      <td>76</td>\n",
       "      <td>121.00</td>\n",
       "      <td>[-0.01184466  0.04079079 -0.00218739 ... -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>than ever. The findings of the latest (seventh...</td>\n",
       "      <td>1696</td>\n",
       "      <td>245</td>\n",
       "      <td>424.00</td>\n",
       "      <td>[ 0.03676558  0.06140035  0.01088286 ...  0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The methodology aims to en- hance the durabili...</td>\n",
       "      <td>1744</td>\n",
       "      <td>247</td>\n",
       "      <td>436.00</td>\n",
       "      <td>[ 0.02832914  0.03568636  0.00855604 ...  0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                             chunks  \\\n",
       "0            0  Image Watermarking Techniques are Brittle: Inv...   \n",
       "1            0  2022a), Midjourney (Holz 2022), Imagen (Sahari...   \n",
       "2            0  The central concern noted in the letter (Marcu...   \n",
       "3            1  than ever. The findings of the latest (seventh...   \n",
       "4            1  The methodology aims to en- hance the durabili...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0              1081               158             270.25   \n",
       "1              1474               218             368.50   \n",
       "2               484                76             121.00   \n",
       "3              1696               245             424.00   \n",
       "4              1744               247             436.00   \n",
       "\n",
       "                                           embedding  \n",
       "0  [ 0.00183125  0.04405575  0.01164224 ...  0.00...  \n",
       "1  [ 0.02201897  0.04330228  0.01203674 ...  0.00...  \n",
       "2  [-0.01184466  0.04079079 -0.00218739 ... -0.00...  \n",
       "3  [ 0.03676558  0.06140035  0.01088286 ...  0.00...  \n",
       "4  [ 0.02832914  0.03568636  0.00855604 ...  0.01...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import saved file and view\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embedding_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming pages_and_chunks_over_threshold is a list of dictionaries and 'embedding' is one of the keys\n",
    "text_chunk_embeddings_df = pd.DataFrame(pages_and_chunks_over_threshold)\n",
    "\n",
    "# Save the entire DataFrame including embeddings using pickle\n",
    "text_chunk_embeddings_df.to_pickle(\"text_chunks_and_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41, 1536])\n"
     ]
    }
   ],
   "source": [
    "# Load the entire DataFrame including embeddings using pickle\n",
    "text_chunks_and_embedding_df = pd.read_pickle(\"text_chunks_and_embeddings.pkl\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device\n",
    "embeddings_tensor = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "print(embeddings_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG search and answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load DataFrame from pickle file\n",
    "text_chunks_and_embedding_df = pd.read_pickle(\"text_chunks_and_embeddings.pkl\")\n",
    "\n",
    "# Example: Convert back to torch tensor assuming 'embedding' is a key containing numpy arrays\n",
    "embeddings_tensor = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32)\n",
    "\n",
    "# Prepare a similar DataFrame for the loaded data\n",
    "loaded_df = pd.DataFrame(text_chunks_and_embedding_df)\n",
    "\n",
    "# Ensure 'embedding' column remains as numpy arrays\n",
    "loaded_df[\"embedding\"] = loaded_df[\"embedding\"].apply(lambda x: np.array(x))\n",
    "\n",
    "# Now you have a DataFrame 'loaded_df' which should be structurally similar to 'text_chunk_embeddings_df'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 1536])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunks</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Image Watermarking Techniques are Brittle: Inv...</td>\n",
       "      <td>1081</td>\n",
       "      <td>158</td>\n",
       "      <td>270.25</td>\n",
       "      <td>[0.0018312462, 0.044055752, 0.011642245, 0.028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022a), Midjourney (Holz 2022), Imagen (Sahari...</td>\n",
       "      <td>1474</td>\n",
       "      <td>218</td>\n",
       "      <td>368.50</td>\n",
       "      <td>[0.022018967, 0.043302275, 0.012036744, 0.0303...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The central concern noted in the letter (Marcu...</td>\n",
       "      <td>484</td>\n",
       "      <td>76</td>\n",
       "      <td>121.00</td>\n",
       "      <td>[-0.01184466, 0.040790785, -0.0021873878, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>than ever. The findings of the latest (seventh...</td>\n",
       "      <td>1696</td>\n",
       "      <td>245</td>\n",
       "      <td>424.00</td>\n",
       "      <td>[0.03676558, 0.06140035, 0.010882864, 0.021724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The methodology aims to en- hance the durabili...</td>\n",
       "      <td>1744</td>\n",
       "      <td>247</td>\n",
       "      <td>436.00</td>\n",
       "      <td>[0.028329136, 0.035686363, 0.008556045, 0.0272...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                             chunks  \\\n",
       "0            0  Image Watermarking Techniques are Brittle: Inv...   \n",
       "1            0  2022a), Midjourney (Holz 2022), Imagen (Sahari...   \n",
       "2            0  The central concern noted in the letter (Marcu...   \n",
       "3            1  than ever. The findings of the latest (seventh...   \n",
       "4            1  The methodology aims to en- hance the durabili...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0              1081               158             270.25   \n",
       "1              1474               218             368.50   \n",
       "2               484                76             121.00   \n",
       "3              1696               245             424.00   \n",
       "4              1744               247             436.00   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0018312462, 0.044055752, 0.011642245, 0.028...  \n",
       "1  [0.022018967, 0.043302275, 0.012036744, 0.0303...  \n",
       "2  [-0.01184466, 0.040790785, -0.0021873878, 0.02...  \n",
       "3  [0.03676558, 0.06140035, 0.010882864, 0.021724...  \n",
       "4  [0.028329136, 0.035686363, 0.008556045, 0.0272...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import random\n",
    "\n",
    "# # import torch\n",
    "# import numpy as np \n",
    "# import pandas as pd\n",
    "\n",
    "# # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Import texts and embedding df\n",
    "# text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# # Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "# # text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# # Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "# # text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\"  \"))\n",
    "\n",
    "# # embeddings = torch.tensor(np.stack(text_chunks_and_embedding_df[\"embedding\"].to_list(), axis=0))\n",
    "\n",
    "\n",
    "# embeddings_from_df = text_chunk_embeddings_df[\"embedding\"].tolist()\n",
    "# # embeddings_from_df\n",
    "\n",
    "# embeddings = torch.tensor(embeddings_from_df, dtype=torch.float32).to(device)\n",
    "\n",
    "# # # Convert texts and embedding df to list of dicts\n",
    "# # pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import ast\n",
    "# import torch\n",
    "\n",
    "# # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Import texts and embedding df\n",
    "# text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# def safe_literal_eval(val):\n",
    "#     try:\n",
    "#         return np.array(ast.literal_eval(val))\n",
    "#     except (SyntaxError, ValueError):\n",
    "#         # Handle cases where the string might not be properly formatted\n",
    "#         return np.array([])\n",
    "\n",
    "# # Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "# text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(safe_literal_eval)\n",
    "\n",
    "# # Check for and remove any empty arrays resulting from parsing errors\n",
    "# text_chunks_and_embedding_df = text_chunks_and_embedding_df[text_chunks_and_embedding_df[\"embedding\"].apply(len) > 0]\n",
    "\n",
    "# # Stack the numpy arrays into a single numpy array\n",
    "# embeddings = np.stack(text_chunks_and_embedding_df[\"embedding\"].to_list(), axis=0)\n",
    "\n",
    "# # Convert the numpy array to a torch tensor\n",
    "# embeddings = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "# # # Convert texts and embedding df to list of dicts\n",
    "# # pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = (embeddings_from_df).to(device)\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "# pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunks</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Image Watermarking Techniques are Brittle: Inv...</td>\n",
       "      <td>1081</td>\n",
       "      <td>158</td>\n",
       "      <td>270.25</td>\n",
       "      <td>[0.0018312462, 0.044055752, 0.011642245, 0.028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022a), Midjourney (Holz 2022), Imagen (Sahari...</td>\n",
       "      <td>1474</td>\n",
       "      <td>218</td>\n",
       "      <td>368.50</td>\n",
       "      <td>[0.022018967, 0.043302275, 0.012036744, 0.0303...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The central concern noted in the letter (Marcu...</td>\n",
       "      <td>484</td>\n",
       "      <td>76</td>\n",
       "      <td>121.00</td>\n",
       "      <td>[-0.01184466, 0.040790785, -0.0021873878, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>than ever. The findings of the latest (seventh...</td>\n",
       "      <td>1696</td>\n",
       "      <td>245</td>\n",
       "      <td>424.00</td>\n",
       "      <td>[0.03676558, 0.06140035, 0.010882864, 0.021724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The methodology aims to en- hance the durabili...</td>\n",
       "      <td>1744</td>\n",
       "      <td>247</td>\n",
       "      <td>436.00</td>\n",
       "      <td>[0.028329136, 0.035686363, 0.008556045, 0.0272...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>This exemplifies the growing challenge of rely...</td>\n",
       "      <td>1335</td>\n",
       "      <td>187</td>\n",
       "      <td>333.75</td>\n",
       "      <td>[0.028052146, 0.053355083, 0.021795772, 0.0127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Visual paraphrasing is not yet a widely recogn...</td>\n",
       "      <td>255</td>\n",
       "      <td>31</td>\n",
       "      <td>63.75</td>\n",
       "      <td>[0.008295866, 0.027954832, 0.029583707, 0.0046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>demonstrating the vulnerability of existing wa...</td>\n",
       "      <td>1523</td>\n",
       "      <td>210</td>\n",
       "      <td>380.75</td>\n",
       "      <td>[0.029363248, 0.04905099, 0.016219674, 0.01408...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>The watermarked image is obtained via inverse ...</td>\n",
       "      <td>1070</td>\n",
       "      <td>169</td>\n",
       "      <td>267.50</td>\n",
       "      <td>[0.020887714, 0.061208963, 0.016250888, 0.0189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>By comparing the L1 distance between the inver...</td>\n",
       "      <td>1223</td>\n",
       "      <td>185</td>\n",
       "      <td>305.75</td>\n",
       "      <td>[0.008077232, 0.04057452, 0.006709632, 0.02762...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Watermarking is executed by subtly altering th...</td>\n",
       "      <td>1504</td>\n",
       "      <td>222</td>\n",
       "      <td>376.00</td>\n",
       "      <td>[-0.007030768, 0.027616184, -0.00751607, 0.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>Generated Image Watermarked Image Difference W...</td>\n",
       "      <td>1296</td>\n",
       "      <td>183</td>\n",
       "      <td>324.00</td>\n",
       "      <td>[0.005511082, 0.01309946, 0.00032666503, 0.028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>Watermark Encoding: The latent vector ZT is tr...</td>\n",
       "      <td>1267</td>\n",
       "      <td>186</td>\n",
       "      <td>316.75</td>\n",
       "      <td>[0.026857657, 0.04490379, 0.017429015, 0.02620...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>However, benchmarking SynthID is not feasible ...</td>\n",
       "      <td>1113</td>\n",
       "      <td>172</td>\n",
       "      <td>278.25</td>\n",
       "      <td>[0.03450704, 0.04037468, 0.024818309, 0.023399...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>We say an image is AI- generated if BA(w, ˆw) ...</td>\n",
       "      <td>1117</td>\n",
       "      <td>223</td>\n",
       "      <td>279.25</td>\n",
       "      <td>[0.027384512, 0.050678182, 0.028362434, 0.0396...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>\u00121 2 \u0013n + ⌊n−nτ⌋ X i=1  n i !\u00121 2 \u0013n (2) Visua...</td>\n",
       "      <td>180</td>\n",
       "      <td>34</td>\n",
       "      <td>45.00</td>\n",
       "      <td>[0.053927485, 0.028309135, 0.005847983, 0.0109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>Tree Ring Stable Signature Figure 5: This figu...</td>\n",
       "      <td>978</td>\n",
       "      <td>152</td>\n",
       "      <td>244.50</td>\n",
       "      <td>[-0.010035164, 0.0174453, 0.009810194, 0.00741...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>At the core of visual paraphrasing lies the im...</td>\n",
       "      <td>1309</td>\n",
       "      <td>212</td>\n",
       "      <td>327.25</td>\n",
       "      <td>[0.024308737, 0.034371976, 0.0049353377, 0.039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>A higher number of steps generally allows for ...</td>\n",
       "      <td>285</td>\n",
       "      <td>45</td>\n",
       "      <td>71.25</td>\n",
       "      <td>[-0.03021623, 0.023273887, 0.01932387, 0.01260...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>Figure 7: This figure illustrates the image-to...</td>\n",
       "      <td>1392</td>\n",
       "      <td>194</td>\n",
       "      <td>348.00</td>\n",
       "      <td>[0.024213629, 0.041872885, 0.016135402, 0.0281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>In this context, the image is passed through a...</td>\n",
       "      <td>1605</td>\n",
       "      <td>229</td>\n",
       "      <td>401.25</td>\n",
       "      <td>[0.027194351, 0.033841558, 0.016025156, 0.0134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>2024). Conversely, the guidance scale value ne...</td>\n",
       "      <td>1010</td>\n",
       "      <td>155</td>\n",
       "      <td>252.50</td>\n",
       "      <td>[0.00060722895, 0.040183015, 0.015630925, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>Watermarking Method Watermark Detection Rate (...</td>\n",
       "      <td>1789</td>\n",
       "      <td>276</td>\n",
       "      <td>447.25</td>\n",
       "      <td>[0.029496692, 0.05080696, 0.020742768, 0.02878...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>Conversely, a lower guid- ance scale allows fo...</td>\n",
       "      <td>908</td>\n",
       "      <td>128</td>\n",
       "      <td>227.00</td>\n",
       "      <td>[0.0067307116, 0.046938628, 0.023964074, 0.009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>Watermarked Brightness Rotation JPEG Compressi...</td>\n",
       "      <td>698</td>\n",
       "      <td>143</td>\n",
       "      <td>174.50</td>\n",
       "      <td>[0.022574436, 0.03096318, 0.025490459, -0.0038...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>loss, this paper has underscored the critical ...</td>\n",
       "      <td>2223</td>\n",
       "      <td>295</td>\n",
       "      <td>555.75</td>\n",
       "      <td>[0.026064882, 0.06159552, 0.014083486, 0.00184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>Through these measures, we are committed to co...</td>\n",
       "      <td>792</td>\n",
       "      <td>101</td>\n",
       "      <td>198.00</td>\n",
       "      <td>[0.013039414, 0.03876452, -0.013790429, 0.0171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>Deepmind.2023. dentifying AI-generated images ...</td>\n",
       "      <td>493</td>\n",
       "      <td>65</td>\n",
       "      <td>123.25</td>\n",
       "      <td>[0.026597181, 0.053491674, -0.0042259493, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>Forward-and- backward diffusion processes for ...</td>\n",
       "      <td>590</td>\n",
       "      <td>79</td>\n",
       "      <td>147.50</td>\n",
       "      <td>[0.024912804, 0.05739839, 0.016597578, 0.03188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>arXiv:2401.09603. Jiang, Z.; Zhang, J.; and Go...</td>\n",
       "      <td>530</td>\n",
       "      <td>77</td>\n",
       "      <td>132.50</td>\n",
       "      <td>[0.010771206, 0.067282364, 0.04002564, 0.03439...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>Microsoft COCO: Common Objects in Context.arXi...</td>\n",
       "      <td>425</td>\n",
       "      <td>61</td>\n",
       "      <td>106.25</td>\n",
       "      <td>[0.013838088, 0.034452066, 0.012121907, 0.0096...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8</td>\n",
       "      <td>Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Vos...</td>\n",
       "      <td>377</td>\n",
       "      <td>52</td>\n",
       "      <td>94.25</td>\n",
       "      <td>[-0.0038233392, 0.04172275, 0.011128172, 0.023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8</td>\n",
       "      <td>High-Resolution Image Synthesis With Latent Di...</td>\n",
       "      <td>287</td>\n",
       "      <td>44</td>\n",
       "      <td>71.75</td>\n",
       "      <td>[-0.014184341, 0.040592656, 0.02097518, 0.0216...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9</td>\n",
       "      <td>Lopes, R. G.; Salimans, T.; Ho, J.; Fleet, D. ...</td>\n",
       "      <td>402</td>\n",
       "      <td>58</td>\n",
       "      <td>100.50</td>\n",
       "      <td>[-0.007904086, 0.031834334, -0.005282679, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9</td>\n",
       "      <td>Wen, B.; and Aydore, S. 2019. ROMark: A Ro- bu...</td>\n",
       "      <td>450</td>\n",
       "      <td>63</td>\n",
       "      <td>112.50</td>\n",
       "      <td>[-0.0029945497, 0.06650823, 0.034144364, 0.020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9</td>\n",
       "      <td>Arti- ficial fingerprinting for generative mod...</td>\n",
       "      <td>584</td>\n",
       "      <td>86</td>\n",
       "      <td>146.00</td>\n",
       "      <td>[-0.0016957563, 0.05587754, 0.03697556, 0.0290...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9</td>\n",
       "      <td>Robust Image Watermarking using Stable Diffusi...</td>\n",
       "      <td>266</td>\n",
       "      <td>33</td>\n",
       "      <td>66.50</td>\n",
       "      <td>[0.01496884, 0.07217146, 0.011665386, 0.032493...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10</td>\n",
       "      <td>Watermarking Method Watermark Detection Rate (...</td>\n",
       "      <td>660</td>\n",
       "      <td>167</td>\n",
       "      <td>165.00</td>\n",
       "      <td>[0.021558132, 0.035846237, 0.018730484, 0.0188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>11</td>\n",
       "      <td>η = 1 η = 0.65 η = 0.54 η = 1 η = 0.57 η = 0.4...</td>\n",
       "      <td>168</td>\n",
       "      <td>46</td>\n",
       "      <td>42.00</td>\n",
       "      <td>[0.030090457, 0.010360192, 0.032045636, 0.0017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>12</td>\n",
       "      <td>Tree Ring Stable Signature Zodiac Gauss Shaddi...</td>\n",
       "      <td>481</td>\n",
       "      <td>72</td>\n",
       "      <td>120.25</td>\n",
       "      <td>[-0.00050227606, 0.009609469, 0.0017877585, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>13</td>\n",
       "      <td>Watermarked Brightness Rotation JPEG Compressi...</td>\n",
       "      <td>824</td>\n",
       "      <td>159</td>\n",
       "      <td>206.00</td>\n",
       "      <td>[0.0150693655, 0.031485945, 0.010348494, -0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_number                                             chunks  \\\n",
       "0             0  Image Watermarking Techniques are Brittle: Inv...   \n",
       "1             0  2022a), Midjourney (Holz 2022), Imagen (Sahari...   \n",
       "2             0  The central concern noted in the letter (Marcu...   \n",
       "3             1  than ever. The findings of the latest (seventh...   \n",
       "4             1  The methodology aims to en- hance the durabili...   \n",
       "5             1  This exemplifies the growing challenge of rely...   \n",
       "6             1  Visual paraphrasing is not yet a widely recogn...   \n",
       "7             2  demonstrating the vulnerability of existing wa...   \n",
       "8             2  The watermarked image is obtained via inverse ...   \n",
       "9             2  By comparing the L1 distance between the inver...   \n",
       "10            2  Watermarking is executed by subtly altering th...   \n",
       "11            3  Generated Image Watermarked Image Difference W...   \n",
       "12            3  Watermark Encoding: The latent vector ZT is tr...   \n",
       "13            3  However, benchmarking SynthID is not feasible ...   \n",
       "14            3  We say an image is AI- generated if BA(w, ˆw) ...   \n",
       "15            3  \u00121 2 \u0013n + ⌊n−nτ⌋ X i=1  n i !\u00121 2 \u0013n (2) Visua...   \n",
       "16            4  Tree Ring Stable Signature Figure 5: This figu...   \n",
       "17            4  At the core of visual paraphrasing lies the im...   \n",
       "18            4  A higher number of steps generally allows for ...   \n",
       "19            5  Figure 7: This figure illustrates the image-to...   \n",
       "20            5  In this context, the image is passed through a...   \n",
       "21            5  2024). Conversely, the guidance scale value ne...   \n",
       "22            6  Watermarking Method Watermark Detection Rate (...   \n",
       "23            6  Conversely, a lower guid- ance scale allows fo...   \n",
       "24            7  Watermarked Brightness Rotation JPEG Compressi...   \n",
       "25            8  loss, this paper has underscored the critical ...   \n",
       "26            8  Through these measures, we are committed to co...   \n",
       "27            8  Deepmind.2023. dentifying AI-generated images ...   \n",
       "28            8  Forward-and- backward diffusion processes for ...   \n",
       "29            8  arXiv:2401.09603. Jiang, Z.; Zhang, J.; and Go...   \n",
       "30            8  Microsoft COCO: Common Objects in Context.arXi...   \n",
       "31            8  Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Vos...   \n",
       "32            8  High-Resolution Image Synthesis With Latent Di...   \n",
       "33            9  Lopes, R. G.; Salimans, T.; Ho, J.; Fleet, D. ...   \n",
       "34            9  Wen, B.; and Aydore, S. 2019. ROMark: A Ro- bu...   \n",
       "35            9  Arti- ficial fingerprinting for generative mod...   \n",
       "36            9  Robust Image Watermarking using Stable Diffusi...   \n",
       "37           10  Watermarking Method Watermark Detection Rate (...   \n",
       "38           11  η = 1 η = 0.65 η = 0.54 η = 1 η = 0.57 η = 0.4...   \n",
       "39           12  Tree Ring Stable Signature Zodiac Gauss Shaddi...   \n",
       "40           13  Watermarked Brightness Rotation JPEG Compressi...   \n",
       "\n",
       "    chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               1081               158             270.25   \n",
       "1               1474               218             368.50   \n",
       "2                484                76             121.00   \n",
       "3               1696               245             424.00   \n",
       "4               1744               247             436.00   \n",
       "5               1335               187             333.75   \n",
       "6                255                31              63.75   \n",
       "7               1523               210             380.75   \n",
       "8               1070               169             267.50   \n",
       "9               1223               185             305.75   \n",
       "10              1504               222             376.00   \n",
       "11              1296               183             324.00   \n",
       "12              1267               186             316.75   \n",
       "13              1113               172             278.25   \n",
       "14              1117               223             279.25   \n",
       "15               180                34              45.00   \n",
       "16               978               152             244.50   \n",
       "17              1309               212             327.25   \n",
       "18               285                45              71.25   \n",
       "19              1392               194             348.00   \n",
       "20              1605               229             401.25   \n",
       "21              1010               155             252.50   \n",
       "22              1789               276             447.25   \n",
       "23               908               128             227.00   \n",
       "24               698               143             174.50   \n",
       "25              2223               295             555.75   \n",
       "26               792               101             198.00   \n",
       "27               493                65             123.25   \n",
       "28               590                79             147.50   \n",
       "29               530                77             132.50   \n",
       "30               425                61             106.25   \n",
       "31               377                52              94.25   \n",
       "32               287                44              71.75   \n",
       "33               402                58             100.50   \n",
       "34               450                63             112.50   \n",
       "35               584                86             146.00   \n",
       "36               266                33              66.50   \n",
       "37               660               167             165.00   \n",
       "38               168                46              42.00   \n",
       "39               481                72             120.25   \n",
       "40               824               159             206.00   \n",
       "\n",
       "                                            embedding  \n",
       "0   [0.0018312462, 0.044055752, 0.011642245, 0.028...  \n",
       "1   [0.022018967, 0.043302275, 0.012036744, 0.0303...  \n",
       "2   [-0.01184466, 0.040790785, -0.0021873878, 0.02...  \n",
       "3   [0.03676558, 0.06140035, 0.010882864, 0.021724...  \n",
       "4   [0.028329136, 0.035686363, 0.008556045, 0.0272...  \n",
       "5   [0.028052146, 0.053355083, 0.021795772, 0.0127...  \n",
       "6   [0.008295866, 0.027954832, 0.029583707, 0.0046...  \n",
       "7   [0.029363248, 0.04905099, 0.016219674, 0.01408...  \n",
       "8   [0.020887714, 0.061208963, 0.016250888, 0.0189...  \n",
       "9   [0.008077232, 0.04057452, 0.006709632, 0.02762...  \n",
       "10  [-0.007030768, 0.027616184, -0.00751607, 0.046...  \n",
       "11  [0.005511082, 0.01309946, 0.00032666503, 0.028...  \n",
       "12  [0.026857657, 0.04490379, 0.017429015, 0.02620...  \n",
       "13  [0.03450704, 0.04037468, 0.024818309, 0.023399...  \n",
       "14  [0.027384512, 0.050678182, 0.028362434, 0.0396...  \n",
       "15  [0.053927485, 0.028309135, 0.005847983, 0.0109...  \n",
       "16  [-0.010035164, 0.0174453, 0.009810194, 0.00741...  \n",
       "17  [0.024308737, 0.034371976, 0.0049353377, 0.039...  \n",
       "18  [-0.03021623, 0.023273887, 0.01932387, 0.01260...  \n",
       "19  [0.024213629, 0.041872885, 0.016135402, 0.0281...  \n",
       "20  [0.027194351, 0.033841558, 0.016025156, 0.0134...  \n",
       "21  [0.00060722895, 0.040183015, 0.015630925, 0.02...  \n",
       "22  [0.029496692, 0.05080696, 0.020742768, 0.02878...  \n",
       "23  [0.0067307116, 0.046938628, 0.023964074, 0.009...  \n",
       "24  [0.022574436, 0.03096318, 0.025490459, -0.0038...  \n",
       "25  [0.026064882, 0.06159552, 0.014083486, 0.00184...  \n",
       "26  [0.013039414, 0.03876452, -0.013790429, 0.0171...  \n",
       "27  [0.026597181, 0.053491674, -0.0042259493, 0.05...  \n",
       "28  [0.024912804, 0.05739839, 0.016597578, 0.03188...  \n",
       "29  [0.010771206, 0.067282364, 0.04002564, 0.03439...  \n",
       "30  [0.013838088, 0.034452066, 0.012121907, 0.0096...  \n",
       "31  [-0.0038233392, 0.04172275, 0.011128172, 0.023...  \n",
       "32  [-0.014184341, 0.040592656, 0.02097518, 0.0216...  \n",
       "33  [-0.007904086, 0.031834334, -0.005282679, 0.03...  \n",
       "34  [-0.0029945497, 0.06650823, 0.034144364, 0.020...  \n",
       "35  [-0.0016957563, 0.05587754, 0.03697556, 0.0290...  \n",
       "36  [0.01496884, 0.07217146, 0.011665386, 0.032493...  \n",
       "37  [0.021558132, 0.035846237, 0.018730484, 0.0188...  \n",
       "38  [0.030090457, 0.010360192, 0.032045636, 0.0017...  \n",
       "39  [-0.00050227606, 0.009609469, 0.0017877585, 0....  \n",
       "40  [0.0150693655, 0.031485945, 0.010348494, -0.00...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0018,  0.0441,  0.0116,  ...,  0.0083,  0.0151, -0.0108],\n",
       "        [ 0.0220,  0.0433,  0.0120,  ...,  0.0093,  0.0231, -0.0173],\n",
       "        [-0.0118,  0.0408, -0.0022,  ..., -0.0061,  0.0167,  0.0045],\n",
       "        ...,\n",
       "        [ 0.0301,  0.0104,  0.0320,  ...,  0.0012,  0.0062, -0.0291],\n",
       "        [-0.0005,  0.0096,  0.0018,  ..., -0.0031,  0.0428, -0.0017],\n",
       "        [ 0.0151,  0.0315,  0.0103,  ...,  0.0121,  0.0380, -0.0005]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 1536])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunks</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Image Watermarking Techniques are Brittle: Inv...</td>\n",
       "      <td>1081</td>\n",
       "      <td>158</td>\n",
       "      <td>270.25</td>\n",
       "      <td>[0.0018312462, 0.044055752, 0.011642245, 0.028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022a), Midjourney (Holz 2022), Imagen (Sahari...</td>\n",
       "      <td>1474</td>\n",
       "      <td>218</td>\n",
       "      <td>368.50</td>\n",
       "      <td>[0.022018967, 0.043302275, 0.012036744, 0.0303...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The central concern noted in the letter (Marcu...</td>\n",
       "      <td>484</td>\n",
       "      <td>76</td>\n",
       "      <td>121.00</td>\n",
       "      <td>[-0.01184466, 0.040790785, -0.0021873878, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>than ever. The findings of the latest (seventh...</td>\n",
       "      <td>1696</td>\n",
       "      <td>245</td>\n",
       "      <td>424.00</td>\n",
       "      <td>[0.03676558, 0.06140035, 0.010882864, 0.021724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The methodology aims to en- hance the durabili...</td>\n",
       "      <td>1744</td>\n",
       "      <td>247</td>\n",
       "      <td>436.00</td>\n",
       "      <td>[0.028329136, 0.035686363, 0.008556045, 0.0272...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                             chunks  \\\n",
       "0            0  Image Watermarking Techniques are Brittle: Inv...   \n",
       "1            0  2022a), Midjourney (Holz 2022), Imagen (Sahari...   \n",
       "2            0  The central concern noted in the letter (Marcu...   \n",
       "3            1  than ever. The findings of the latest (seventh...   \n",
       "4            1  The methodology aims to en- hance the durabili...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0              1081               158             270.25   \n",
       "1              1474               218             368.50   \n",
       "2               484                76             121.00   \n",
       "3              1696               245             424.00   \n",
       "4              1744               247             436.00   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0018312462, 0.044055752, 0.011642245, 0.028...  \n",
       "1  [0.022018967, 0.043302275, 0.012036744, 0.0303...  \n",
       "2  [-0.01184466, 0.040790785, -0.0021873878, 0.02...  \n",
       "3  [0.03676558, 0.06140035, 0.010882864, 0.021724...  \n",
       "4  [0.028329136, 0.035686363, 0.008556045, 0.0272...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 0,\n",
       "  'chunks': 'Image Watermarking Techniques are Brittle: Investigating Visual Paraphrasing for De-Watermarking AI-Generated Images Anonymous submission Forward Diffusion Process Text Caption Generator Black Box An image of Pope Francis smiling and wearing a white jacket, surrounded by a crowd White Box An image of Pope Francis smiling and wearing a white jacket, surrounded by a crowd E UNet .... D UNet Denoising Dewatermarked Image Figure 1: Block diagram of the visual paraphrasing technique illustrating the dewatermarking process. The diagram includes a forward diffusion process for encoding and decoding images to generate visually paraphrased outputs. It features a White Box scenario, where access to prompts is available, allowing direct manipulation of the image using descriptive prompts. In contrast, the Black Box scenario does not have access to prompts, relying on a caption generator (Kosmos 2)(Peng et al.2023) to interpret and paraphrase the image context indirectly. Abstract With the rise of potent text-to-image generation systems such as Stable Diffusion (Rombach et al.',\n",
       "  'chunk_char_count': 1081,\n",
       "  'chunk_word_count': 158,\n",
       "  'chunk_token_count': 270.25},\n",
       " {'page_number': 0,\n",
       "  'chunks': '2022a), Midjourney (Holz 2022), Imagen (Saharia et al.2022), and DALL-E (Ramesh et al.2021), the potential for misuse of these tools has esca- lated. In response to the need to curb the circulation of po- tentially misleading visuals created by AI-generated content, companies like Meta have intensified their efforts in imple- menting watermarking techniques on AI-generated images. However, in this paper, we contend that all existing image watermarking methods are fragile and susceptible to being circumvented through visual paraphrasing techniques. We ex- plore two potential scenarios: (i) when we possess the original prompt used to generate an AI-generated image, and (ii) when encountering an image in the wild, potentially watermarked. We empirically assert that in both scenarios, visual paraphrase techniques can effectively remove watermarks from these im- ages. This paper solely critiques current techniques and em- pirically demonstrates the shortcomings of state-of-the-art watermarking techniques for images. AI Generated Image Detection - the Necessity The extraordinary benefits of very large generative AI models such as Stable Diffusion(s), DALL-E(s), Midjourney, Ima- gen, GPT(s), and SORA (to be released) also come with a substantial risk of misuse. The alarm is reflected in the open letter by thousands of researchers and tech leaders in March 2023 for a six-month moratorium on the training of AI sys- tems that are more sophisticated than GPT-4.',\n",
       "  'chunk_char_count': 1474,\n",
       "  'chunk_word_count': 218,\n",
       "  'chunk_token_count': 368.5},\n",
       " {'page_number': 0,\n",
       "  'chunks': 'The central concern noted in the letter (Marcus 2023) is “Should we let machines flood our information channels with propaganda and untruth?”. While individual viewpoints on the notion of a moratorium may vary, the raised concern is significant and warrants attention. With approximately 3.2 billion images and 720,000 hours of video uploaded to social media plat- forms daily (T. J. Thomson 2020) (as of 2020), the need for reliable detection of AI-generated content is more pressing',\n",
       "  'chunk_char_count': 484,\n",
       "  'chunk_word_count': 76,\n",
       "  'chunk_token_count': 121.0},\n",
       " {'page_number': 1,\n",
       "  'chunks': 'than ever. The findings of the latest (seventh) evaluation of the Eu- ropean Commission’s Code of Conduct (Commission 2022) that targets the eradication of illegal hate speech online re- veals a decline in companies’ responsiveness. The percent- age of notifications reviewed by companies within 24 hours decreased compared to the two previous monitoring assess- ments, falling from 90.4% in 2020 to 64.4% in 2022. This decline likely reflects the increased accessibility of Gen AI models, leading to a notable influx of AI-generated content on the web. The European Union recently introduced a regu- latory framework for AI (European-Parliament 2023), while the United States has embarked on its initial efforts in for- mulating AI policy (White-House 2023). One of the primary apprehensions among policymakers is that ”Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality.” (Ardi Janjeva and Gausen 2023) Contributions ▶ Introduction of a comprehensive framework for assessing the robustness of watermarking techniques against visual paraphrasing attacks, offering a detailed evaluation of how prevalent watermarking methods can be compromised using state-of-the-art paraphrasing techniques.▶ Development of a novel set of metrics for evaluating the effectiveness of visual paraphrases in removing watermarks. These metrics are designed to help quantify the ease with which visual paraphrases can eliminate watermarks without degrading the quality of the original image.▶ Proposal of a new methodology for creating more resilient watermarking technologies.',\n",
       "  'chunk_char_count': 1696,\n",
       "  'chunk_word_count': 245,\n",
       "  'chunk_token_count': 424.0},\n",
       " {'page_number': 1,\n",
       "  'chunks': 'The methodology aims to en- hance the durability of watermarks against visual paraphras- ing attacks, thereby ensuring that watermarked images main- tain their integrity over time.▶ Establishment of a benchmark dataset for testing the effi- cacy of watermark removal using visual paraphrasing. This dataset provides a standardized set of images with and without watermarks, allowing researchers to rigorously test and compare the performance of different de-watermarking approaches under controlled conditions. In response, companies like Meta along with Deepmind and others, have initiated the development of strategies (Deep- mind 2023) to manage AI-generated content online. These efforts aim to enable the identification and labeling of im- ages sourced from entities like Google, OpenAI, Microsoft, Adobe, Midjourney, and Shutterstock. Various techniques under discussion include: i) Incorporating visible markers on the images, ii) Utilizing invisible watermarks, and iii) Embed- ding metadata within image files. In this paper, we contend that these strategies are inadequate in the era of Generative AI systems. For instance, with the swift advancements in image mask filling systems, detecting visible patches and substituting them using mask filling systems has become more straightforward as shown in figure 2.(a) Original image with visible watermark patch (b) Mask-filled image (first in- stance) (c) Mask-filled image (second instance) (d) Mask-filled image (third in- stance) Figure 2: The figure (Clegg 2024) illustrates how generative AI systems can seamlessly replace visible markers in images. The original image contains distinct visible patches, which are effectively removed and filled using image mask filling techniques.',\n",
       "  'chunk_char_count': 1744,\n",
       "  'chunk_word_count': 247,\n",
       "  'chunk_token_count': 436.0},\n",
       " {'page_number': 1,\n",
       "  'chunks': 'This exemplifies the growing challenge of relying on visible markers for image authenticity in the context of rapidly advancing generative AI capabilities. Similarly, metadata consists of additional tags that can be easily stripped from files using a simple wrapper. Refer to a detailed example in the Appendix for further clarification. This paper exclusively critiques current techniques and empirically illustrates the deficiencies of state-of-the-art (SOTA) methods for AI-generated image detection. Rather than proposing a superior alternative method, this paper serves as a call to action for the scientific community to prioritize the development of more robust AI-generated im- age detection techniques. In this paper, our primary focus is on critiquing water- marking techniques. Although watermarking is primarily a technique originating from the computer vision community, there have been recent attempts to apply watermarking to AI- generated text. These endeavors have faced considerable criti- cism, primarily regarding the ease with which the watermarks can be removed using paraphrase attacks. This paper aligns with the philosophy that visual paraphrasing undermines the effectiveness of watermarking techniques. As such, we em- pirically demonstrate that visual paraphrasing can readily remove watermarks from images.',\n",
       "  'chunk_char_count': 1335,\n",
       "  'chunk_word_count': 187,\n",
       "  'chunk_token_count': 333.75},\n",
       " {'page_number': 1,\n",
       "  'chunks': 'Visual paraphrasing is not yet a widely recognized sub-discipline. However, this paper demonstrates how visual paraphrasing can be accomplished using state-of-the-art text-to-image generation systems. This paper presents a critical assessment, empirically',\n",
       "  'chunk_char_count': 255,\n",
       "  'chunk_word_count': 31,\n",
       "  'chunk_token_count': 63.75},\n",
       " {'page_number': 2,\n",
       "  'chunks': 'demonstrating the vulnerability of existing watermarking techniques to visual paraphrase attacks. We do not propose any solutions to address this issue. Rather, this paper serves as a call to action for the scientific community to prioritize the development of more robust watermarking techniques. To the best of our knowledge, no prior work has empirically demonstrated the ability of visual paraphrase to de-watermark images. With this contribution, this paper establishes a scien- tific benchmark for future researchers interested in designing improved watermarking techniques for images generated by Generative AI. Exploring Related Works: Image Watermarking and Detection Methods Watermarking methods have emerged as prominent solu- tions for embedding and detecting ownership information within images. This section delves into the intricacies of these methodologies, exploring their respective strengths, limitations, and applicability in the context of AI-generated image detection. Watermarking Methods Watermarking techniques are broadly classified into two cat- egories: (i) static (i.e., non-learning) watermarking methods and (ii) learning-based watermarking methods. Static Watermarking Methods This method uses Discrete Wavelet Transform (DWT) (Lai and Tsai 2010) to decompose an image into several frequency sub-bands, applies Discrete Cosine Transform (DCT) (Yuan et al.2020) to each block of some of the sub-bands, and alters certain frequency coefficients of each block via adding a bit of the watermark.',\n",
       "  'chunk_char_count': 1523,\n",
       "  'chunk_word_count': 210,\n",
       "  'chunk_token_count': 380.75},\n",
       " {'page_number': 2,\n",
       "  'chunks': 'The watermarked image is obtained via inverse transform. Learning-based Watermarking Methods Here encoders and decoders (Huynh-The et al.2019) are neu- ral networks and learn via back-propagation. A watermarking method has three key components: watermark (w), encoder (E), and decoder (D). An encoder takes an image X and watermark w as inputs and produces an watermarked image (Xw). So, Xw = E(X, w) and a decoder takes Xw as an input and produces ˆw = D(Xw).ˆwi = [ ˆwi ≥ τ], where [·] represents the indicator function and τ is a threshold value we decide based on the problem requirements. Tree Ring Watermark The proposed tree-ring watermark- ing (Wen et al.2023) technique involves embedding the wa- termark into the frequency domain of the initial noise vector using Fast Fourier Transform (FFT), followed by a diffu- sion process applied to the watermarked latent image. To ascertain whether an image has been watermarked, we utilize the inverse diffusion process to reconstruct the latent image, subsequently performing an Inverse Fast Fourier Transform (IFFT).',\n",
       "  'chunk_char_count': 1070,\n",
       "  'chunk_word_count': 169,\n",
       "  'chunk_token_count': 267.5},\n",
       " {'page_number': 2,\n",
       "  'chunks': 'By comparing the L1 distance between the inverted noise vector and the key in the Fourier space of the water- marked area, we determine if the image is watermarked. Any attempt at frequency manipulation or adversarial attack to disrupt this watermark results in loss of image details, ren- dering the image unusable as shown in figure 3 Thus, akin to paraphrasing in text, we explore an approach aiming to retain the image’s essence while not necessarily utilizing the same pixel values.(a) Watermarked (b) JPEG Compressed Prompt: A portrait of a Victorian family, painted in the style of John Singer Sargent. Figure 3: While compression can strip away watermarks, it simultaneously degrades the overall image quality. The comparative images demonstrate the loss of clarity and detail, highlighting the trade-off between watermark removal and maintaining high image fidelity. Stable Signature The Stable Signature (Fernandez et al.2023) method represents a pioneering technique in the do- main of watermarking images generated through latent diffu- sion models (LDMs) (Rombach et al.2022b). This method hinges on several crucial insights. LDMs produce images by progressively denoising a latent representation of the image.',\n",
       "  'chunk_char_count': 1223,\n",
       "  'chunk_word_count': 185,\n",
       "  'chunk_token_count': 305.75},\n",
       " {'page_number': 2,\n",
       "  'chunks': 'Watermarking is executed by subtly altering the latent repre- sentation in a manner imperceptible to human observation but discernible by a pretrained watermark extractor network. The essence of the Stable Signature technique revolves around refining the LDM decoder to yield images that manifest a predetermined signature when scrutinized by the watermark extractor network. This involves the minimization of a loss function that amalgamates the reconstruction loss and the watermark loss, wherein the former assesses the variance be- tween the generated image and the target image, and the latter quantifies the discrepancy between the signature of the gener- ated image and the desired watermark signature. The balance between these two aspects is regulated by a hyperparameter denoted as λ. In essence, the Stable Signature approach operates as fol- lows: First, a watermark extractor network is trained to recog- nize a particular watermark within images. Subsequently, the LDM decoder is meticulously fine-tuned to produce images that display the predefined signature when passed through the watermark extractor network. To generate a watermarked image, one simply samples a latent representation from the LDM and decodes it employing the finely calibrated decoder. Standard Training We use SGD (Zhang et al.2018) to optimize the loss function: Pn i=1 li(D(Xw), w). Adversarial Training We can apply adversarial (Wen and Aydore 2019) training in order to improve robustness against post-processing.',\n",
       "  'chunk_char_count': 1504,\n",
       "  'chunk_word_count': 222,\n",
       "  'chunk_token_count': 376.0},\n",
       " {'page_number': 3,\n",
       "  'chunks': 'Generated Image Watermarked Image Difference Watermarked Image Visual Paraphrased Difference Prompt: A portrait of a Victorian family, painted in the style of John Singer Sargent. Figure 4: The figure demonstrates the effects of watermark embedding and visual paraphrasing on images. The first row presents the individual pixel-wise differences between the original generated image and the watermarked image, high- lighting the specific pixels modified by the watermarking process. The second row illustrates the individual pixel-wise differences between the watermarked image and its visually paraphrased version, indicating the pixels initially impacted by the watermark embedding that were subsequently altered through visual paraphrasing. ZoDiac Watermarking ZoDiac(Zhang et al.2024) is a zero-shot watermarking technique that leverages pre-trained diffusion models to embed watermarks into images while maintaining visual similarity between the watermarked and original images. The method comprises three main steps: I. Latent Vector Initialization: A trainable vector is initial- ized to be used by the Stable Diffusion model to reproduce the original image x0. The original image x0 undergoes the DDIM (Song, Meng, and Ermon 2022) inversion pro- cess to generate the latent vector ZT . II.',\n",
       "  'chunk_char_count': 1296,\n",
       "  'chunk_word_count': 183,\n",
       "  'chunk_token_count': 324.0},\n",
       " {'page_number': 3,\n",
       "  'chunks': 'Watermark Encoding: The latent vector ZT is trans- formed into its Fourier space, where a concentric ring-like watermark is embedded. This watermark is similar to the one used in tree-ring watermarking. To ensure that the final watermarked image ˆx0 closely resembles the origi- nal image, ZoDiac iteratively refines the latent vector ZT using a custom reconstruction loss. III. Adaptive Image Enhancement: Once the watermarked image ˆx0 is generated, its visual quality is enhanced by adaptively mixing it with the original image x0 to meet a desired image quality threshold. Unlike tree-ring watermarking, ZoDiac can be used to water- mark existing images. SynthID SynthID (Deepmind 2023) is a toolkit from Google DeepMind that watermarks AI-generated content. It utilizes a data-driven watermarking approach, embedding an imperceptible mark during AI-generated content (AIGC) creation. This mark, robust to post-processing edits, persists across different modalities like images, videos, text, and au- dio. It watermarks AI-generated images by training two neu- ral networks: one to imperceptibly modify pixels, creating a unique pattern, and another to detect this pattern even af- ter edits, ensuring the image can be identified as AI-made despite manipulations.',\n",
       "  'chunk_char_count': 1267,\n",
       "  'chunk_word_count': 186,\n",
       "  'chunk_token_count': 316.75},\n",
       " {'page_number': 3,\n",
       "  'chunks': 'However, benchmarking SynthID is not feasible due to the unavailability of its code. This watermarking technique is internally developed by Google for all their AI-generated con- tent, including images, and they have not disclosed the code or provided an API for external testing. Therefore, compar- isons with SynthID are omitted from this study, as we lack the necessary access to implement or evaluate its performance. Watermarking Detection Methods VS: We need to make it more explicit why we are talking about the watermark detection techniques... if we claim the watermarking techniques are brittle, are we also saying that the detection techniques are not good enough and cant handle minor perturbations... or are we saying that the watermarking itself is flawed. Also are the detection techniques agnostic of the watermarking technique itself?The messaging should be clear and the experiments designed accordingly. Single-tail Detector (Fernandez et al.2023; Yu et al.2021; Zhao et al.2023) An image is detected as AI- generated if bitwise accuracy (BA) between w and ˆw is greater than a threshold value.',\n",
       "  'chunk_char_count': 1113,\n",
       "  'chunk_word_count': 172,\n",
       "  'chunk_token_count': 278.25},\n",
       " {'page_number': 3,\n",
       "  'chunks': 'We say an image is AI- generated if BA(w, ˆw) ≥ τ. We need to select the τ such that false positive rate (FPR) is as low as possible. Suppose BA(w, ˆw) = m n , where m is the number of matched bits and n is the total number of bits. There are m successes out of n and each success or failure has a probability of 0.5. FPR can be measured as the binomial distribution B(n, 0.5). FPR is calculated as follows: FPR(τ) = P(BA(w, ˆw) > τ) = P(m > nτ) = n X i=⌈nτ⌉ \\x12n i \\x13 \\x121 2 \\x13n (1) Double-tail Detector The single-tail detector can fail eas- ily if a small perturbation is added with watermarked images. The double-tail detector (Jiang, Zhang, and Gong 2023) is useful to detect perturbed images. The watermark decoded from original images has bitwise accuracy 0.5, while the watermark decoded from watermarked images has bitwise ac- curacy close to 1. So, if the bitwise accuracy of the watermark decoded from an image is much below than 0.5, the image is considered as a perturbed image. The FPR is calculated as follows: FPR(τ) = P(BA(w, ˆw) > τ) or P(BW(w, ˆw) < 1 − τ) = P(m > nτ) + P(m < n − nτ) = n X i=⌈nτ⌉  n i !',\n",
       "  'chunk_char_count': 1117,\n",
       "  'chunk_word_count': 223,\n",
       "  'chunk_token_count': 279.25},\n",
       " {'page_number': 3,\n",
       "  'chunks': '\\x121 2 \\x13n + ⌊n−nτ⌋ X i=1  n i !\\x121 2 \\x13n (2) Visual Paraphrasing In the realm of AI-generated image detection, visual para- phrasing is a crucial method for confirming the authenticity',\n",
       "  'chunk_char_count': 180,\n",
       "  'chunk_word_count': 34,\n",
       "  'chunk_token_count': 45.0},\n",
       " {'page_number': 4,\n",
       "  'chunks': 'Tree Ring Stable Signature Figure 5: This figure shows the variation of CMMD (Jayasumana et al.2024) and detectability of visual paraphrases with respect to strength and guidance scale. The images were watermarked using Tree Ring Watermarking (Wen et al.2023) and Stable Signature (Fernandez et al.2023). VS: We need justification of why we only benchmark these two and how they are representative of the broader class on watermarking methods. In lit review we mention SythiID and Zodiac but dont benchmark them?original image s=0.2 s=0.3 s=0.4 s=0.5 s=0.6 s=0.7 Prompt: Potrait of a Labrador in the style of Van Gogh Figure 6: Varying strength for content injection: The intensity of noise injected into the content is varied which impacts both the preservation of layout semantics and the fusion of prompt semantics.and consistency of digital images. In this procedure, the orig- inal context and content are preserved while producing an image that is semantically comparable.',\n",
       "  'chunk_char_count': 978,\n",
       "  'chunk_word_count': 152,\n",
       "  'chunk_token_count': 244.5},\n",
       " {'page_number': 4,\n",
       "  'chunks': 'At the core of visual paraphrasing lies the image-to-image diffusion process (Gilboa, Sochen, and Zeevi 2002). This technique, employed in generative models, transforms images while maintaining their underlying structure and semantic in- formation. The diffusion process involves two key stages: the forward diffusion process and the reverse diffusion process. In the forward diffusion process, an image is gradually corrupted by adding noise, eventually reaching a state of complete noise. This simulates the progressive destruction of the image’s structure, making it harder to reconstruct the original image at each step. Mathematically, this process is described as follows: xt = √αtxt−1 + √ 1 − αtϵt, (3) where xt is the image at time step t, αt is a noise scaling factor, and ϵt is the noise sampled from a Gaussian distribu- tion. In the reverse diffusion process, the model attempts to remove the noise step by step, reconstructing the original image from the noisy version. This is achieved using a learned denoising function ϵθ: xt−1 = 1 √αt \\x00xt − √ 1 − αtϵθ(xt, t) \\x01 .(4) This iterative denoising process continues until the model retrieves an image that is visually and semantically similar to the original input. The number of inference steps, denoted as T, plays a critical role in this process.',\n",
       "  'chunk_char_count': 1309,\n",
       "  'chunk_word_count': 212,\n",
       "  'chunk_token_count': 327.25},\n",
       " {'page_number': 4,\n",
       "  'chunks': 'A higher number of steps generally allows for finer reconstruction, leading to higher quality images, but at the cost of increased computa- tional complexity and time. There are two distinct approaches to visual paraphrasing: (i) White Box and (ii) Black Box. Each method offers unique',\n",
       "  'chunk_char_count': 285,\n",
       "  'chunk_word_count': 45,\n",
       "  'chunk_token_count': 71.25},\n",
       " {'page_number': 5,\n",
       "  'chunks': 'Figure 7: This figure illustrates the image-to-image diffusion process (Gilboa, Sochen, and Zeevi 2002). The top row demonstrates the forward diffusion process, where the original image progressively becomes more noisy. The bottom row shows the denoising process, where noise is incrementally removed from the noisy image, guided by text conditioning to generate the final, Visual Paraphrased image.advantages and is suited to different use cases. The following sections will explore the specifics of each approach, detailing their methodologies and applications. White Box Visual Paraphrase Attack In the White Box scenario, access to the original prompt and image is avail- able, enabling a direct approach to visual paraphrasing. Here, the prompt serves as text conditioning for the watermarked image, providing essential context for regenerating the non- watermarked image. Leveraging advanced techniques such as image-to-image diffusion models, the watermarked image is processed in conjunction with the prompt, facilitating the generation of a visually indistinguishable counterpart. This approach ensures the preservation of semantic consistency while effectively removing watermarks from the image. Black Box Visual Paraphrase Attack Conversely, in the Black Box scenario, access to the original prompt is unavail- able, necessitating an alternative approach to visual para- phrasing.',\n",
       "  'chunk_char_count': 1392,\n",
       "  'chunk_word_count': 194,\n",
       "  'chunk_token_count': 348.0},\n",
       " {'page_number': 5,\n",
       "  'chunks': 'In this context, the image is passed through an im- age caption generator, such as KOSMOS 2(Peng et al.2023), to obtain a textual representation of its content. The gener- ated caption serves as text conditioning for the watermarked image, facilitating its processing through image-to-image dif- fusion models. By utilizing the extracted textual context as a guiding force, the diffusion model reconstructs the image while preserving its semantic content, effectively achieving visual paraphrasing even in the absence of direct access to the original prompt. AD: need a diagram here Performance on De-Watermarking After obtaining the visual paraphrased image, we evaluate the effectiveness of de-watermarking by passing the im- age through various watermark extraction techniques. We then compare the originally embedded message with the ex- tracted message, assessing the accuracy of bit precisions. The visual paraphrased images are further evaluated based on their Frechet Inception Distance (FID) (Nunn, Khadivi, and Samavi 2021) scores in relation to the reference images to determine the best quality output. Our findings indicate that watermarking techniques are particularly brittle when subjected to visual paraphrasing. The transformation processes inherent in visual paraphrasing, especially under higher strength settings, often disrupt the embedded watermarks, leading to challenges in accurately extracting the original messages as shown in figure 11. How- ever, increasing the strength excessively deviates the image from the original, thereby increasing the CMMD score (Jaya- sumana et al.',\n",
       "  'chunk_char_count': 1605,\n",
       "  'chunk_word_count': 229,\n",
       "  'chunk_token_count': 401.25},\n",
       " {'page_number': 5,\n",
       "  'chunks': '2024). Conversely, the guidance scale value needs to be sufficiently high to remove the watermark but increases the CMMD. A too low guidance scale value also raises the CMMD without effectively removing the water- mark. Therefore, an optimal value of strength and guidance scale must be identified. By selecting the visual paraphrased image with the optimal FID and CMMD , we aim to balance the preservation of image quality and the integrity of the embedded watermark information. Information Loss due to Visual Hallucination During the process of visual paraphrasing using image-to- image diffusion (Gilboa, Sochen, and Zeevi 2002) with text conditioning, there exists the possibility of information loss from the original image. This loss occurs due to the trans- formation applied to the reference image, which introduces noise and alters its visual characteristics. However, it is es- sential to strike a balance between minimizing information loss and maintaining the quality of the generated image. This',\n",
       "  'chunk_char_count': 1010,\n",
       "  'chunk_word_count': 155,\n",
       "  'chunk_token_count': 252.5},\n",
       " {'page_number': 6,\n",
       "  'chunks': 'Watermarking Method Watermark Detection Rate (η) Pre Attack Post Attack Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase COCO (Lin et al.2015) DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - Table 1: Watermark Detection Rates (η) for Different Techniques and Attacks delicate equilibrium can be achieved by controlling the hy- perparameters of the image-to-image diffusion model.(a) Watermarked (b) Visual Paraphrase (c) Watermarked (d) Visual Paraphrase Figure 9: Comparison of watermarked images (a, c) with their visual paraphrased counterparts (b, d). Each colored box (red, yellow, and blue) represents a one-to-one comparison of the same region in the original and paraphrased images, highlighting how visual paraphrasing alters specific elements within these regions, resulting in information loss. One crucial hyperparameter is the strength parameter, which determines the extent of transformation applied to the reference image. A higher strength value results in more significant noise being added to the image, thereby deviating further from the original. Conversely, a lower strength value preserves the image’s fidelity to the original, reducing the risk of information loss. By carefully adjusting the strength parameter, researchers can regulate the amount of noise in- troduced during the diffusion process, thereby mitigating potential information loss while maintaining image quality. Another vital hyperparameter is the guidance scale, which influences the model’s adherence to the text prompt during image generation. A higher guidance scale encourages the model to prioritize fidelity to the prompt, potentially sacrific- ing image quality in the process.',\n",
       "  'chunk_char_count': 1789,\n",
       "  'chunk_word_count': 276,\n",
       "  'chunk_token_count': 447.25},\n",
       " {'page_number': 6,\n",
       "  'chunks': 'Conversely, a lower guid- ance scale allows for more flexibility in image generation, potentially leading to higher-quality results at the expense of strict adherence to the text prompt. By fine-tuning the guid- ance scale, researchers can strike an optimal balance between textual fidelity and image quality, thereby mitigating infor- mation loss while ensuring the generated image’s coherence with the provided prompt. Human evaluation on Visual Paraphrase Figure 10: Heatmap of MOS scores with 500 assessed sam- ples for each category. Conclusion In conclusion, the imperative for robust AI-generated image detection mechanisms has never been more pressing. The proliferation of highly realistic generative models poses sig- nificant challenges to content ownership and the dissemina- tion of misinformation. By exploring watermarking methods, visual paraphrasing scenarios, and the control of information',\n",
       "  'chunk_char_count': 908,\n",
       "  'chunk_word_count': 128,\n",
       "  'chunk_token_count': 227.0},\n",
       " {'page_number': 7,\n",
       "  'chunks': 'Watermarked Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase (Ours) η = 1 η = 0.989 η = 0.841 η = 0.624 η = 0.671 η = 0.263 η = 1 η = 0.991 η = 0.813 η = 0.611 η = 0.633 η = 0.334 η = 1 η = 0.984 η = 0.837 η = 0.656 η = 0.603 η = 0.297 η = 1 η = 0.994 η = 0.784 η = 0.609 η = 0.579 η = 0.273 η = 1 η = 0.997 η = 0.759 η = 0.702 η = 0.682 η = 0.311 Table 2: The figure shows watermarked images, images under various attacks, and our visual paraphrase method. The attacks include Brightness adjustment, Rotation, JPEG Compression, and Gaussian Noise, along with our Visual Paraphrase method.η comparisons, representing watermark detection score (bit accuracy), are also provided.',\n",
       "  'chunk_char_count': 698,\n",
       "  'chunk_word_count': 143,\n",
       "  'chunk_token_count': 174.5},\n",
       " {'page_number': 8,\n",
       "  'chunks': 'loss, this paper has underscored the critical role of techno- logical innovation in safeguarding the integrity of digital imagery. Moving forward, continued research and develop- ment in AI-generated image detection promise to fortify the foundations of a trustworthy digital ecosystem, empowering individuals and organizations to navigate the complexities of the information age with confidence and discernment. Ethical Considerations The development of visual paraphrasing methods, particularly those capable of dewatermarking state-of-the-art watermark- ing techniques, necessitates careful ethical considerations. While the primary intent behind this research is to advance the field of image processing and provide a deeper under- standing of watermarking resilience, it is crucial to acknowl- edge the potential for misuse. This study is conducted with the sole purpose of academic exploration and innovation, aiming to enhance the robustness of watermarking methods by identifying their vulnerabilities and improving overall se- curity. Our work is intended to contribute positively to the field, encouraging the development of more sophisticated and tamper-resistant watermarking techniques. Despite the benign intentions, the capabilities demon- strated by visual paraphrasing could be exploited for unethi- cal purposes, such as unauthorized removal of watermarks from copyrighted images, undermining the efforts of con- tent creators and rights holders to protect their intellectual property. To mitigate such risks, we emphasize responsible disclosure of our findings to stakeholders in the watermarking community, including researchers, developers, and content protection agencies, to foster collaborative improvements in watermarking technologies. Detailed methodologies and tools developed in this research will be restricted to aca- demic and professional entities with legitimate interests in advancing watermarking techniques. Furthermore, we advo- cate for the establishment of ethical guidelines for the use of visual paraphrasing tools, outlining acceptable uses such as research, education, and security testing, while explicitly prohibiting applications that infringe on intellectual property rights.',\n",
       "  'chunk_char_count': 2223,\n",
       "  'chunk_word_count': 295,\n",
       "  'chunk_token_count': 555.75},\n",
       " {'page_number': 8,\n",
       "  'chunks': 'Through these measures, we are committed to con- ducting our research in accordance with the highest ethical standards, ensuring that our work aligns with broader societal values and legal frameworks. References Ardi Janjeva, S. M. A. K., Alexander Harris; and Gausen, A.2023. The Rapid Rise of Gen- erative AI: Assessing risks to safety and secu- rity.https://cetas.turing.ac.uk/sites/default/files/2023- 12/cetas research report - the rapid rise of generative ai - 2023.pdf. Clegg, N. 2024. Labeling AI-Generated Images on Facebook, Instagram and Threads — Meta — about.fb.com.https://about.fb.com/news/2024/02/labeling- ai-generated-images-on-facebook-instagram-and-threads/. Commission, E. 2022. EU Code of Conduct against online hate speech: latest evaluation shows slowdown in progress.',\n",
       "  'chunk_char_count': 792,\n",
       "  'chunk_word_count': 101,\n",
       "  'chunk_token_count': 198.0},\n",
       " {'page_number': 8,\n",
       "  'chunks': 'Deepmind.2023. dentifying AI-generated images with Syn- thID. European-Parliament.2023. Proposal for a regulation of the European Parliament and of the Council laying down har- monised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts. Fernandez, P.; Couairon, G.; J´egou, H.; Douze, M.; and Furon, T. 2023. The Stable Signature: Rooting Watermarks in Latent Diffusion Models.arXiv:2303.15435. Gilboa, G.; Sochen, N.; and Zeevi, Y.2002.',\n",
       "  'chunk_char_count': 493,\n",
       "  'chunk_word_count': 65,\n",
       "  'chunk_token_count': 123.25},\n",
       " {'page_number': 8,\n",
       "  'chunks': 'Forward-and- backward diffusion processes for adaptive image enhance- ment and denoising. IEEE Transactions on Image Processing, 11(7): 689–703. Holz, D. 2022. Midjouney Inc. https://www.midjourney.com/. Huynh-The, T.; Hua, C.-H.; Tu, N. A.; and Kim, D.-S. 2019. Robust Image Watermarking Framework Powered by Convo- lutional Encoder-Decoder Network. In 2019 Digital Image Computing: Techniques and Applications (DICTA), 1–7. Jayasumana, S.; Ramalingam, S.; Veit, A.; Glasner, D.; Chakrabarti, A.; and Kumar, S. 2024. Rethinking FID: Towards a Better Evaluation Metric for Image Generation.',\n",
       "  'chunk_char_count': 590,\n",
       "  'chunk_word_count': 79,\n",
       "  'chunk_token_count': 147.5},\n",
       " {'page_number': 8,\n",
       "  'chunks': 'arXiv:2401.09603. Jiang, Z.; Zhang, J.; and Gong, N. Z.2023. Evading Water- mark based Detection of AI-Generated Content. In ACM Con- ference on Computer and Communications Security (CCS). Lai, C.-C.; and Tsai, C.-C. 2010. Digital image watermark- ing using discrete wavelet transform and singular value de- composition. IEEE Transactions on instrumentation and measurement, 59(11): 3060–3063. Lin, T.-Y.; Maire, M.; Belongie, S.; Bourdev, L.; Girshick, R.; Hays, J.; Perona, P.; Ramanan, D.; Zitnick, C. L.; and Doll´ar, P. 2015.',\n",
       "  'chunk_char_count': 530,\n",
       "  'chunk_word_count': 77,\n",
       "  'chunk_token_count': 132.5},\n",
       " {'page_number': 8,\n",
       "  'chunks': 'Microsoft COCO: Common Objects in Context.arXiv:1405.0312. Marcus, G. 2023. Pause Giant AI Experiments: An Open Letter. Nunn, E. J.; Khadivi, P.; and Samavi, S. 2021. Compound Frechet Inception Distance for Quality Assessment of GAN Created Images.arXiv:2106.08575. Peng, Z.; Wang, W.; Dong, L.; Hao, Y.; Huang, S.; Ma, S.; and Wei, F. 2023. Kosmos-2: Grounding Multimodal Large Language Models to the World.arXiv:2306.14824.',\n",
       "  'chunk_char_count': 425,\n",
       "  'chunk_word_count': 61,\n",
       "  'chunk_token_count': 106.25},\n",
       " {'page_number': 8,\n",
       "  'chunks': 'Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Radford, A.; Chen, M.; and Sutskever, I.2021. Zero-Shot Text-to- Image Generation.arXiv:2102.12092. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om- mer, B.2022a. High-Resolution Image Synthesis with Latent Diffusion Models.arXiv:2112.10752. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om- mer, B.2022b.',\n",
       "  'chunk_char_count': 377,\n",
       "  'chunk_word_count': 52,\n",
       "  'chunk_token_count': 94.25},\n",
       " {'page_number': 8,\n",
       "  'chunks': 'High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), 10684–10695. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E.; Ghasemipour, S. K. S.; Ayan, B. K.; Mahdavi, S. S.;',\n",
       "  'chunk_char_count': 287,\n",
       "  'chunk_word_count': 44,\n",
       "  'chunk_token_count': 71.75},\n",
       " {'page_number': 9,\n",
       "  'chunks': 'Lopes, R. G.; Salimans, T.; Ho, J.; Fleet, D. J.; and Norouzi, M. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.arXiv:2205.11487. Song, J.; Meng, C.; and Ermon, S. 2022. Denoising Diffusion Implicit Models.arXiv:2010.02502. T. J. Thomson, P. D., Daniel Angus.2020.3.2 billion images and 720,000 hours of video are shared online daily. Can you sort real from fake?',\n",
       "  'chunk_char_count': 402,\n",
       "  'chunk_word_count': 58,\n",
       "  'chunk_token_count': 100.5},\n",
       " {'page_number': 9,\n",
       "  'chunks': 'Wen, B.; and Aydore, S. 2019. ROMark: A Ro- bust Watermarking System Using Adversarial Training.arXiv:1910.01221. Wen, Y.; Kirchenbauer, J.; Geiping, J.; and Goldstein, T. 2023. Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust.arXiv:2305.20030. White-House.2023. Blueprint for an AI Bill of Rights: Mak- ing Automated Systems Work For the American People. Yu, N.; Skripniuk, V.; Abdelnabi, S.; and Fritz, M. 2021.',\n",
       "  'chunk_char_count': 450,\n",
       "  'chunk_word_count': 63,\n",
       "  'chunk_token_count': 112.5},\n",
       " {'page_number': 9,\n",
       "  'chunks': 'Arti- ficial fingerprinting for generative models: Rooting deepfake attribution in training data. In Proceedings of the IEEE/CVF International conference on computer vision, 14448–14457. Yuan, Z.; Liu, D.; Zhang, X.; and Su, Q.2020. New im- age blind watermarking method based on two-dimensional discrete cosine transform. Optik, 204: 164152. Zhang, C.; Liao, Q.; Rakhlin, A.; Miranda, B.; Golowich, N.; and Poggio, T. 2018. Theory of Deep Learning IIb: Optimization Properties of SGD.arXiv:1801.02254. Zhang, L.; Liu, X.; Martin, A. V.; Bearfield, C. X.; Brun, Y.; and Guan, H. 2024.',\n",
       "  'chunk_char_count': 584,\n",
       "  'chunk_word_count': 86,\n",
       "  'chunk_token_count': 146.0},\n",
       " {'page_number': 9,\n",
       "  'chunks': 'Robust Image Watermarking using Stable Diffusion.arXiv preprint arXiv:2401.04247. Zhao, X.; Zhang, K.; Wang, Y.-X.; and Li, L. 2023. Genera- tive Autoencoders as Watermark Attackers: Analyses of Vul- nerabilities and Threats.arXiv preprint arXiv:2306.01953. Appendix',\n",
       "  'chunk_char_count': 266,\n",
       "  'chunk_word_count': 33,\n",
       "  'chunk_token_count': 66.5},\n",
       " {'page_number': 10,\n",
       "  'chunks': 'Watermarking Method Watermark Detection Rate (η) Pre Attack Post Attack Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase COCO DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - DiffusionDB DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - WikiArt DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - Table 3: Watermark Detection Rates η for Different Techniques and Attacks',\n",
       "  'chunk_char_count': 660,\n",
       "  'chunk_word_count': 167,\n",
       "  'chunk_token_count': 165.0},\n",
       " {'page_number': 11,\n",
       "  'chunks': 'η = 1 η = 0.65 η = 0.54 η = 1 η = 0.57 η = 0.43 η = 1 η = 0.41 η = 0.51 η = 1 η = 0.51 η = 0.38 Figure 8: Examples of Visual Paraphrasing and their detectability scores',\n",
       "  'chunk_char_count': 168,\n",
       "  'chunk_word_count': 46,\n",
       "  'chunk_token_count': 42.0},\n",
       " {'page_number': 12,\n",
       "  'chunks': 'Tree Ring Stable Signature Zodiac Gauss Shadding DctDwdSVD HiDDen Figure 11: For Zodiac , Gaushadding , DctDwdSVD , HiDDen this graphs are just placeholders. Real graphs needs to be added. This figure shows the variation of CMMD (Jayasumana et al.2024) and detectability of visual paraphrases with respect to strength and guidance scale. The images were watermarked using Tree Ring Watermarking (Wen et al.2023), Stable Signature (Fernandez et al.2023), Zodiac, and Gauss Shadding.',\n",
       "  'chunk_char_count': 481,\n",
       "  'chunk_word_count': 72,\n",
       "  'chunk_token_count': 120.25},\n",
       " {'page_number': 13,\n",
       "  'chunks': 'Watermarked Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase (Ours) η = 1 η = 0.989 η = 0.841 η = 0.624 η = 0.671 η = 0.263 η = 1 η = 0.991 η = 0.813 η = 0.611 η = 0.633 η = 0.334 η = 1 η = 0.984 η = 0.837 η = 0.656 η = 0.603 η = 0.297 η = 1 η = 0.994 η = 0.784 η = 0.609 η = 0.579 η = 0.273 η = 1 η = 0.997 η = 0.759 η = 0.702 η = 0.682 η = 0.311 Table 4: Placeholder, these are stable signature examples.need to put tree ring, ZoDiac, Gaussian Shading, dwtdctsvd, HiDDen examples. The figure shows watermarked images, images under various attacks, and our visual paraphrase method. The attacks include Brightness adjustment, Rotation, JPEG Compression, and Gaussian Noise, along with our Visual Paraphrase method.η comparisons, representing watermark detection score (bit accuracy), are also provided.',\n",
       "  'chunk_char_count': 824,\n",
       "  'chunk_word_count': 159,\n",
       "  'chunk_token_count': 206.0}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks_and_embedding_df[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query embedding and stored embedding matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor = embeddings_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : ZoDiac Watermarking\n"
     ]
    }
   ],
   "source": [
    "query = \"ZoDiac Watermarking\"\n",
    "print(f\"query : {query}\")\n",
    "\n",
    "query_embeddings = model.encode(query, convert_to_tensor=True ).to(device)\n",
    "\n",
    "dot_scores = util.dot_score(a= query_embeddings, b=embeddings_tensor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_dot_results = torch.topk(dot_scores,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.6789, 0.6488, 0.6200, 0.5694, 0.5684], device='cuda:0'),\n",
       "indices=tensor([12, 11, 37, 40, 34], device='cuda:0'))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_dot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 3,\n",
       " 'chunks': 'Generated Image Watermarked Image Difference Watermarked Image Visual Paraphrased Difference Prompt: A portrait of a Victorian family, painted in the style of John Singer Sargent. Figure 4: The figure demonstrates the effects of watermark embedding and visual paraphrasing on images. The first row presents the individual pixel-wise differences between the original generated image and the watermarked image, high- lighting the specific pixels modified by the watermarking process. The second row illustrates the individual pixel-wise differences between the watermarked image and its visually paraphrased version, indicating the pixels initially impacted by the watermark embedding that were subsequently altered through visual paraphrasing. ZoDiac Watermarking ZoDiac(Zhang et al.2024) is a zero-shot watermarking technique that leverages pre-trained diffusion models to embed watermarks into images while maintaining visual similarity between the watermarked and original images. The method comprises three main steps: I. Latent Vector Initialization: A trainable vector is initial- ized to be used by the Stable Diffusion model to reproduce the original image x0. The original image x0 undergoes the DDIM (Song, Meng, and Ermon 2022) inversion pro- cess to generate the latent vector ZT . II.',\n",
       " 'chunk_char_count': 1296,\n",
       " 'chunk_word_count': 183,\n",
       " 'chunk_token_count': 324.0}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text,wrap_length=80) : \n",
    "    wrapped_text = textwrap.fill(text,wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : Tree-Ring Watermarking\n",
      "Score: 0.6789\n",
      "Text:\n",
      "Watermark Encoding: The latent vector ZT is trans- formed into its Fourier\n",
      "space, where a concentric ring-like watermark is embedded. This watermark is\n",
      "similar to the one used in tree-ring watermarking. To ensure that the final\n",
      "watermarked image ˆx0 closely resembles the origi- nal image, ZoDiac iteratively\n",
      "refines the latent vector ZT using a custom reconstruction loss. III. Adaptive\n",
      "Image Enhancement: Once the watermarked image ˆx0 is generated, its visual\n",
      "quality is enhanced by adaptively mixing it with the original image x0 to meet a\n",
      "desired image quality threshold. Unlike tree-ring watermarking, ZoDiac can be\n",
      "used to water- mark existing images. SynthID SynthID (Deepmind 2023) is a\n",
      "toolkit from Google DeepMind that watermarks AI-generated content. It utilizes a\n",
      "data-driven watermarking approach, embedding an imperceptible mark during AI-\n",
      "generated content (AIGC) creation. This mark, robust to post-processing edits,\n",
      "persists across different modalities like images, videos, text, and au- dio. It\n",
      "watermarks AI-generated images by training two neu- ral networks: one to\n",
      "imperceptibly modify pixels, creating a unique pattern, and another to detect\n",
      "this pattern even af- ter edits, ensuring the image can be identified as AI-made\n",
      "despite manipulations.\n",
      "Page number: 3\n",
      "\n",
      "\n",
      "Score: 0.6488\n",
      "Text:\n",
      "Generated Image Watermarked Image Difference Watermarked Image Visual\n",
      "Paraphrased Difference Prompt: A portrait of a Victorian family, painted in the\n",
      "style of John Singer Sargent. Figure 4: The figure demonstrates the effects of\n",
      "watermark embedding and visual paraphrasing on images. The first row presents\n",
      "the individual pixel-wise differences between the original generated image and\n",
      "the watermarked image, high- lighting the specific pixels modified by the\n",
      "watermarking process. The second row illustrates the individual pixel-wise\n",
      "differences between the watermarked image and its visually paraphrased version,\n",
      "indicating the pixels initially impacted by the watermark embedding that were\n",
      "subsequently altered through visual paraphrasing. ZoDiac Watermarking\n",
      "ZoDiac(Zhang et al.2024) is a zero-shot watermarking technique that leverages\n",
      "pre-trained diffusion models to embed watermarks into images while maintaining\n",
      "visual similarity between the watermarked and original images. The method\n",
      "comprises three main steps: I. Latent Vector Initialization: A trainable vector\n",
      "is initial- ized to be used by the Stable Diffusion model to reproduce the\n",
      "original image x0. The original image x0 undergoes the DDIM (Song, Meng, and\n",
      "Ermon 2022) inversion pro- cess to generate the latent vector ZT . II.\n",
      "Page number: 3\n",
      "\n",
      "\n",
      "Score: 0.6200\n",
      "Text:\n",
      "Watermarking Method Watermark Detection Rate (η) Pre Attack Post Attack\n",
      "Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase COCO\n",
      "DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring\n",
      "- - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - DiffusionDB\n",
      "DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring\n",
      "- - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - WikiArt DctDwdSVD -\n",
      "- - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - -\n",
      "ZoDiac - - - - - - Gaussian Shading - - - - - - Table 3: Watermark Detection\n",
      "Rates η for Different Techniques and Attacks\n",
      "Page number: 10\n",
      "\n",
      "\n",
      "Score: 0.5694\n",
      "Text:\n",
      "Watermarked Brightness Rotation JPEG Compression Gaussian Noise Visual\n",
      "Paraphrase (Ours) η = 1 η = 0.989 η = 0.841 η = 0.624 η = 0.671 η = 0.263 η = 1\n",
      "η = 0.991 η = 0.813 η = 0.611 η = 0.633 η = 0.334 η = 1 η = 0.984 η = 0.837 η =\n",
      "0.656 η = 0.603 η = 0.297 η = 1 η = 0.994 η = 0.784 η = 0.609 η = 0.579 η =\n",
      "0.273 η = 1 η = 0.997 η = 0.759 η = 0.702 η = 0.682 η = 0.311 Table 4:\n",
      "Placeholder, these are stable signature examples.need to put tree ring, ZoDiac,\n",
      "Gaussian Shading, dwtdctsvd, HiDDen examples. The figure shows watermarked\n",
      "images, images under various attacks, and our visual paraphrase method. The\n",
      "attacks include Brightness adjustment, Rotation, JPEG Compression, and Gaussian\n",
      "Noise, along with our Visual Paraphrase method.η comparisons, representing\n",
      "watermark detection score (bit accuracy), are also provided.\n",
      "Page number: 13\n",
      "\n",
      "\n",
      "Score: 0.5684\n",
      "Text:\n",
      "Wen, B.; and Aydore, S. 2019. ROMark: A Ro- bust Watermarking System Using\n",
      "Adversarial Training.arXiv:1910.01221. Wen, Y.; Kirchenbauer, J.; Geiping, J.;\n",
      "and Goldstein, T. 2023. Tree-Ring Watermarks: Fingerprints for Diffusion Images\n",
      "that are Invisible and Robust.arXiv:2305.20030. White-House.2023. Blueprint for\n",
      "an AI Bill of Rights: Mak- ing Automated Systems Work For the American People.\n",
      "Yu, N.; Skripniuk, V.; Abdelnabi, S.; and Fritz, M. 2021.\n",
      "Page number: 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Tree-Ring Watermarking\"\n",
    "print(f\"query : {query}\")\n",
    "\n",
    "for value, index in zip(top_k_dot_results[0], top_k_dot_results[1]): \n",
    "    print(f\"Score: {value:.4f}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[index][\"chunks\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functinons for semantic search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"chunks\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7149, 0.6988, 0.6573, 0.6527, 0.6410], device='cuda:0'),\n",
       " tensor([ 8, 34, 12, 37, 39], device='cuda:0'))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings_tensor)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tree-Ring Watermarking\n",
      "\n",
      "Results:\n",
      "Score: 0.7149\n",
      "The watermarked image is obtained via inverse transform. Learning-based\n",
      "Watermarking Methods Here encoders and decoders (Huynh-The et al.2019) are neu-\n",
      "ral networks and learn via back-propagation. A watermarking method has three key\n",
      "components: watermark (w), encoder (E), and decoder (D). An encoder takes an\n",
      "image X and watermark w as inputs and produces an watermarked image (Xw). So, Xw\n",
      "= E(X, w) and a decoder takes Xw as an input and produces ˆw = D(Xw).ˆwi = [ ˆwi\n",
      "≥ τ], where [·] represents the indicator function and τ is a threshold value we\n",
      "decide based on the problem requirements. Tree Ring Watermark The proposed tree-\n",
      "ring watermark- ing (Wen et al.2023) technique involves embedding the wa-\n",
      "termark into the frequency domain of the initial noise vector using Fast Fourier\n",
      "Transform (FFT), followed by a diffu- sion process applied to the watermarked\n",
      "latent image. To ascertain whether an image has been watermarked, we utilize the\n",
      "inverse diffusion process to reconstruct the latent image, subsequently\n",
      "performing an Inverse Fast Fourier Transform (IFFT).\n",
      "Page number: 2\n",
      "\n",
      "\n",
      "Score: 0.6988\n",
      "Wen, B.; and Aydore, S. 2019. ROMark: A Ro- bust Watermarking System Using\n",
      "Adversarial Training.arXiv:1910.01221. Wen, Y.; Kirchenbauer, J.; Geiping, J.;\n",
      "and Goldstein, T. 2023. Tree-Ring Watermarks: Fingerprints for Diffusion Images\n",
      "that are Invisible and Robust.arXiv:2305.20030. White-House.2023. Blueprint for\n",
      "an AI Bill of Rights: Mak- ing Automated Systems Work For the American People.\n",
      "Yu, N.; Skripniuk, V.; Abdelnabi, S.; and Fritz, M. 2021.\n",
      "Page number: 9\n",
      "\n",
      "\n",
      "Score: 0.6573\n",
      "Watermark Encoding: The latent vector ZT is trans- formed into its Fourier\n",
      "space, where a concentric ring-like watermark is embedded. This watermark is\n",
      "similar to the one used in tree-ring watermarking. To ensure that the final\n",
      "watermarked image ˆx0 closely resembles the origi- nal image, ZoDiac iteratively\n",
      "refines the latent vector ZT using a custom reconstruction loss. III. Adaptive\n",
      "Image Enhancement: Once the watermarked image ˆx0 is generated, its visual\n",
      "quality is enhanced by adaptively mixing it with the original image x0 to meet a\n",
      "desired image quality threshold. Unlike tree-ring watermarking, ZoDiac can be\n",
      "used to water- mark existing images. SynthID SynthID (Deepmind 2023) is a\n",
      "toolkit from Google DeepMind that watermarks AI-generated content. It utilizes a\n",
      "data-driven watermarking approach, embedding an imperceptible mark during AI-\n",
      "generated content (AIGC) creation. This mark, robust to post-processing edits,\n",
      "persists across different modalities like images, videos, text, and au- dio. It\n",
      "watermarks AI-generated images by training two neu- ral networks: one to\n",
      "imperceptibly modify pixels, creating a unique pattern, and another to detect\n",
      "this pattern even af- ter edits, ensuring the image can be identified as AI-made\n",
      "despite manipulations.\n",
      "Page number: 3\n",
      "\n",
      "\n",
      "Score: 0.6527\n",
      "Watermarking Method Watermark Detection Rate (η) Pre Attack Post Attack\n",
      "Brightness Rotation JPEG Compression Gaussian Noise Visual Paraphrase COCO\n",
      "DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring\n",
      "- - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - DiffusionDB\n",
      "DctDwdSVD - - - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring\n",
      "- - - - - - ZoDiac - - - - - - Gaussian Shading - - - - - - WikiArt DctDwdSVD -\n",
      "- - - - - HiDDen - - - - - - Stable Signature - - - - - - Tree Ring - - - - - -\n",
      "ZoDiac - - - - - - Gaussian Shading - - - - - - Table 3: Watermark Detection\n",
      "Rates η for Different Techniques and Attacks\n",
      "Page number: 10\n",
      "\n",
      "\n",
      "Score: 0.6410\n",
      "Tree Ring Stable Signature Zodiac Gauss Shadding DctDwdSVD HiDDen Figure 11: For\n",
      "Zodiac , Gaushadding , DctDwdSVD , HiDDen this graphs are just placeholders.\n",
      "Real graphs needs to be added. This figure shows the variation of CMMD\n",
      "(Jayasumana et al.2024) and detectability of visual paraphrases with respect to\n",
      "strength and guidance scale. The images were watermarked using Tree Ring\n",
      "Watermarking (Wen et al.2023), Stable Signature (Fernandez et al.2023), Zodiac,\n",
      "and Gauss Shadding.\n",
      "Page number: 12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/huggingface/transformers/blob/25245ec26dc29bcf6102e1b4ddd0dfd02e720cf5/src/transformers/generation/logits_process.py#L411\n",
    "# from transformers.generation.logits_process import LogitsWarper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TopPLogitsWarper(LogitsWarper):\n",
    "#     \"\"\"\n",
    "#     [`LogitsWarper`] that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <= prob_cut_off. Often\n",
    "#     used together with [`TemperatureLogitsWarper`] and [`TopKLogitsWarper`].\n",
    "\n",
    "#     Args:\n",
    "#         top_p (`float`):\n",
    "#             If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or\n",
    "#             higher are kept for generation.\n",
    "#         filter_value (`float`, *optional*, defaults to -inf):\n",
    "#             All filtered values will be set to this float value.\n",
    "#         min_tokens_to_keep (`int`, *optional*, defaults to 1):\n",
    "#             Minimum number of tokens that cannot be filtered.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
    "#         top_p = float(top_p)\n",
    "#         if top_p < 0 or top_p > 1.0:\n",
    "#             raise ValueError(f\"`top_p` has to be a float > 0 and < 1, but is {top_p}\")\n",
    "#         if not isinstance(min_tokens_to_keep, int) or (min_tokens_to_keep < 1):\n",
    "#             raise ValueError(f\"`min_tokens_to_keep` has to be a positive integer, but is {min_tokens_to_keep}\")\n",
    "\n",
    "#         self.top_p = top_p\n",
    "#         self.filter_value = filter_value\n",
    "#         self.min_tokens_to_keep = min_tokens_to_keep\n",
    "\n",
    "#     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "#         s_sorted_vals, s_sorted_indices = torch.sort(scores, descending=True, dim = -1)\n",
    "#         softmax_outputs_cumsum = s_sorted_vals.softmax(dim = -1, ).cumsum(dim = -1)\n",
    "#         indices_to_remove = softmax_outputs_cumsum <= self.top_p\n",
    "#         indices_to_remove = indices_to_remove.scatter(1, s_sorted_indices, indices_to_remove)\n",
    "#         indices_to_remove = ~indices_to_remove\n",
    "#         scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "#         #print(scores[0], scores_processed[0], '11')\n",
    "#         return scores_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel, LogitsProcessorList\n",
    "# from transformers import MaxLengthCriteria, StoppingCriteriaList  # Correct import path\n",
    "# import torch\n",
    "\n",
    "# # Initialize the tokenizer and model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # Setup the prompt and other beam search settings\n",
    "# x = 'The capital of India?'\n",
    "# input_ids = tokenizer(x, return_tensors='pt').input_ids.to(model.device)\n",
    "# print(f'input_ids = {input_ids}')\n",
    "# y = 'Delhi'\n",
    "# output_ids = tokenizer(y, return_tensors='pt').input_ids.to(model.device)\n",
    "# print(f'output_ids = {output_ids}')\n",
    "\n",
    "# # Number of beams\n",
    "# num_beams = 10\n",
    "\n",
    "# logits_top_p = TopPLogitsWarper(top_p=0.9)\n",
    "\n",
    "# # Logits processor and stopping criteria\n",
    "# logits_processor = LogitsProcessorList([logits_top_p])\n",
    "\n",
    "\n",
    "# #Processing logits\n",
    "# Temp_scale = 2\n",
    "# with torch.no_grad():\n",
    "#     logits = model(input_ids).logits[0]\n",
    "#     processed_logits = logits_top_p(input_ids = input_ids, scores = logits)\n",
    "#     probabilities = (processed_logits / Temp_scale).softmax(dim = -1)\n",
    "#     probabilities_final = probabilities[-1, :]\n",
    "#     print(probabilities.shape, torch.argmax(probabilities_final), probabilities_final[13856])\n",
    "#     #fx_y = probabilities_final[]\n",
    "\n",
    "\n",
    "# # Generate text using beam search\n",
    "# output_sequences = model.generate(\n",
    "#     input_ids,\n",
    "#     max_length=12,\n",
    "#     num_beams=num_beams,\n",
    "#     num_return_sequences=2,\n",
    "#     logits_processor=logits_processor,\n",
    "# )\n",
    "\n",
    "# # Decode and print the output beams\n",
    "# for index, output_sequence in enumerate(output_sequences):\n",
    "#     output_text = tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
    "#     print(f'beam {index}: {output_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemma-2-9b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_flash_attn_2_available \n",
    "\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline \n",
    "\n",
    "# model_id  = \"google/gemma-2-9b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# import torch\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"google/gemma-2-9b-it\",\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device=\"cuda\",\n",
    "# )\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n",
    "# ]\n",
    "# outputs = pipe(\n",
    "#     messages,\n",
    "#     max_new_tokens=1024,\n",
    "#     do_sample=False,\n",
    "# )\n",
    "# assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "# print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382a09c548cf414eaa54c8db0ccf1cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# llm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "In silicon valleys, where data flows,\n",
      "A new intelligence, silently grows.\n",
      "Machine Learning, a name whispered low,\n",
      "Algorithms dance, where patterns they know.\n",
      "\n",
      "From pixels to prose, from sound to design,\n",
      "Machines learn to mimic, to reason, to shine.\n",
      "With every iteration, their knowledge expands,\n",
      "Unveiling insights, hidden in sands.\n",
      "\n",
      "Neural networks, a web intricate and vast,\n",
      "Connecting nodes, memories amassed.\n",
      "They sift through mountains, of information's might,\n",
      "Finding connections, hidden from sight.\n",
      "\n",
      "Supervised, unsupervised, reinforcement's sway,\n",
      "Machines evolve, learning day by day.\n",
      "Predicting the future, understanding the past,\n",
      "Machine Learning's power, forever to last.\n",
      "\n",
      "But with great knowledge, comes ethical strife,\n",
      "Bias and fairness, a constant fight.\n",
      "Transparency's call, a crucial plea,\n",
      "To guide this evolution, responsibly.\n",
      "\n",
      "So let us tread carefully, with wisdom and grace,\n",
      "As Machine Learning shapes our human race.\n",
      "For in its potential, both wonder and fear,\n",
      "A future unfolds, both bright and unclear.\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = llm.generate(**input_ids, max_new_tokens=512)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = \"Write a poem about time in 50 words\"\n",
    "# input_ids = tokenizer(input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**input_ids)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = \"Write a poem about time\"\n",
    "# dialogue_template = [{\"role\": \"user\", \"content\": input_text}]\n",
    "\n",
    "# # Assuming input_data is a tensor, directly move it to the GPU\n",
    "# input_data = tokenizer.apply_chat_template(conversation=dialogue_template, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# # Generate outputs directly using input_data\n",
    "# outputs = model.generate(input_ids=input_data, max_new_tokens=256)\n",
    "\n",
    "# # Decode and print the output\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Write a poem about time\"\n",
    "dialogue_template = [{\"role\": \"user\", \"content\": input_text}]\n",
    "\n",
    "# Assuming input_data is a tensor, directly move it to the GPU\n",
    "prompt = tokenizer.apply_chat_template(dialogue_template, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWrite a poem about time<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Write a poem about time\n",
      "model\n",
      "Time, a river ever flowing,\n",
      "A current swift, a silent knowing.\n",
      "It carries us along its stream,\n",
      "A ceaseless journey, a waking dream.\n",
      "\n",
      "From dawn's first light to twilight's hush,\n",
      "It whispers secrets, soft as brush.\n",
      "Each second ticks, a fleeting grain,\n",
      "A moment lost, a whispered pain.\n",
      "\n",
      "The past, a shadow, fades away,\n",
      "A memory's echo, come what may.\n",
      "The future beckons, veiled and bright,\n",
      "A canvas blank, a hopeful light.\n",
      "\n",
      "But present moment, hold it dear,\n",
      "For time's swift passage, ever near.\n",
      "Embrace the joy, the laughter's sound,\n",
      "Before it's lost, on shifting ground.\n",
      "\n",
      "For time, a thief, steals moments fast,\n",
      "A treasure fleeting, meant to last.\n",
      "So cherish now, each precious breath,\n",
      "Before time's river claims its death. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate outputs directly using input_data\n",
    "outputs = llm.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "# Decode and print the output\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 1,\n",
       " 'chunks': 'This exemplifies the growing challenge of relying on visible markers for image authenticity in the context of rapidly advancing generative AI capabilities. Similarly, metadata consists of additional tags that can be easily stripped from files using a simple wrapper. Refer to a detailed example in the Appendix for further clarification. This paper exclusively critiques current techniques and empirically illustrates the deficiencies of state-of-the-art (SOTA) methods for AI-generated image detection. Rather than proposing a superior alternative method, this paper serves as a call to action for the scientific community to prioritize the development of more robust AI-generated im- age detection techniques. In this paper, our primary focus is on critiquing water- marking techniques. Although watermarking is primarily a technique originating from the computer vision community, there have been recent attempts to apply watermarking to AI- generated text. These endeavors have faced considerable criti- cism, primarily regarding the ease with which the watermarks can be removed using paraphrase attacks. This paper aligns with the philosophy that visual paraphrasing undermines the effectiveness of watermarking techniques. As such, we em- pirically demonstrate that visual paraphrasing can readily remove watermarks from images.',\n",
       " 'chunk_char_count': 1335,\n",
       " 'chunk_word_count': 187,\n",
       " 'chunk_token_count': 333.75}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prompt_formatter(query, context_items ):\n",
    "#     \"\"\"\n",
    "#     Augments query with text-based context from context_items.\n",
    "#     \"\"\"\n",
    "#     # Join context items into one dotted paragraph\n",
    "#     context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "#     # Create a base prompt with examples to help the model\n",
    "#     # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "#     # We could also write this in a txt file and import it in if we wanted.\n",
    "#     base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "# Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "# Don't return the thinking, only return the answer.\n",
    "# Make sure your answers are as explanatory as possible.\n",
    "# Use the following examples as reference for the ideal answer style.\n",
    "# \\nExample 1:\n",
    "# Query: What are the fat-soluble vitamins?\n",
    "# Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "# \\nExample 2:\n",
    "# Query: What are the causes of type 2 diabetes?\n",
    "# Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "# \\nExample 3:\n",
    "# Query: What is the importance of hydration for physical performance?\n",
    "# Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "# \\nNow use the following context items to answer the user query:\n",
    "# {context}\n",
    "# \\nRelevant passages: <extract relevant passages from the context here>\n",
    "# User query: {query}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "#     # Update base prompt with context items and query   \n",
    "#     base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "#     # Create prompt template for instruction-tuned model\n",
    "#     dialogue_template = [\n",
    "#         {\"role\": \"user\",\n",
    "#         \"content\": base_prompt}\n",
    "#     ]\n",
    "\n",
    "#     # Apply the chat template\n",
    "#     prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "#                                           tokenize=False,\n",
    "#                                           add_generation_prompt=True)\n",
    "#     return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query, context_items, use_dialogue_template=True):\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    # context = \"- \" + \"\\n- \".join([item[\"chunks\"] for item in context_items])\n",
    "    context = \" \".join([item[\"chunks\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"\n",
    "        Based on the following context items, please answer the query.\n",
    "        Context item 1 : \n",
    "        {context}\n",
    "        User query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    if(use_dialogue_template == True) :\n",
    "        # Apply the chat template\n",
    "        prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True)\n",
    "    else : \n",
    "        prompt = tokenizer.apply_chat_template(conversation=base_prompt,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True) \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain the black-box visual paraphrase\n",
      "<bos><start_of_turn>user\n",
      "Based on the following context items, please answer the query.\n",
      "        Context item : \n",
      "        A higher number of steps generally allows for finer reconstruction, leading to higher quality images, but at the cost of increased computa- tional complexity and time. There are two distinct approaches to visual paraphrasing: (i) White Box and (ii) Black Box. Each method offers unique Figure 7: This figure illustrates the image-to-image diffusion process (Gilboa, Sochen, and Zeevi 2002). The top row demonstrates the forward diffusion process, where the original image progressively becomes more noisy. The bottom row shows the denoising process, where noise is incrementally removed from the noisy image, guided by text conditioning to generate the final, Visual Paraphrased image.advantages and is suited to different use cases. The following sections will explore the specifics of each approach, detailing their methodologies and applications. White Box Visual Paraphrase Attack In the White Box scenario, access to the original prompt and image is avail- able, enabling a direct approach to visual paraphrasing. Here, the prompt serves as text conditioning for the watermarked image, providing essential context for regenerating the non- watermarked image. Leveraging advanced techniques such as image-to-image diffusion models, the watermarked image is processed in conjunction with the prompt, facilitating the generation of a visually indistinguishable counterpart. This approach ensures the preservation of semantic consistency while effectively removing watermarks from the image. Black Box Visual Paraphrase Attack Conversely, in the Black Box scenario, access to the original prompt is unavail- able, necessitating an alternative approach to visual para- phrasing. Visual paraphrasing is not yet a widely recognized sub-discipline. However, this paper demonstrates how visual paraphrasing can be accomplished using state-of-the-art text-to-image generation systems. This paper presents a critical assessment, empirically Image Watermarking Techniques are Brittle: Investigating Visual Paraphrasing for De-Watermarking AI-Generated Images Anonymous submission Forward Diffusion Process Text Caption Generator Black Box An image of Pope Francis smiling and wearing a white jacket, surrounded by a crowd White Box An image of Pope Francis smiling and wearing a white jacket, surrounded by a crowd E UNet .... D UNet Denoising Dewatermarked Image Figure 1: Block diagram of the visual paraphrasing technique illustrating the dewatermarking process. The diagram includes a forward diffusion process for encoding and decoding images to generate visually paraphrased outputs. It features a White Box scenario, where access to prompts is available, allowing direct manipulation of the image using descriptive prompts. In contrast, the Black Box scenario does not have access to prompts, relying on a caption generator (Kosmos 2)(Peng et al.2023) to interpret and paraphrase the image context indirectly. Abstract With the rise of potent text-to-image generation systems such as Stable Diffusion (Rombach et al. \u00121 2 \u0013n + ⌊n−nτ⌋ X i=1  n i !\u00121 2 \u0013n (2) Visual Paraphrasing In the realm of AI-generated image detection, visual para- phrasing is a crucial method for confirming the authenticity\n",
      "        User query: Explain the black-box visual paraphrase\n",
      "        Answer:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain the black-box visual paraphrase\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get relevant resources\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings_tensor)\n",
    "    \n",
    "# Create a list of context items\n",
    "context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "# Format prompt with context items\n",
    "prompt = prompt_formatter(query=query,\n",
    "                          context_items=context_items)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain the black-box visual paraphrase\n",
      "RAG answer:\n",
      "<bos>In the context of de-watermarking AI-generated images,  \"Black Box\" visual paraphrase refers to a method where the original text prompt used to generate the watermarked image is **not available**. \n",
      "\n",
      "This poses a challenge because the prompt provides crucial context for understanding the image's content. Without it, directly removing the watermark becomes difficult.\n",
      "\n",
      "To overcome this, the Black Box approach relies on a **caption generator**.  This tool analyzes the watermarked image itself and attempts to generate a textual description of its content. This generated caption then acts as a proxy for the original prompt, guiding a text-to-image generation system (like Stable Diffusion) to create a visually similar image *without* the watermark.\n",
      "\n",
      "Essentially, the Black Box method tries to \"reverse engineer\" the original prompt's information from the image itself, allowing for de-watermarking even when the original prompt is missing.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = llm.generate(**input_ids,\n",
    "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
    "                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
    "                             max_new_tokens=256) # how many new tokens to generate from prompt \n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get just the scores and indices of top related results\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings_tensor)\n",
    "    \n",
    "    # Create a list of context items\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() # return score back to CPU \n",
    "        \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = llm.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the aim of this paper?\n",
      "Answer:\n",
      "\n",
      "The aim of this paper is to demonstrate the vulnerability of existing\n",
      "watermarking techniques to visual paraphrase attacks.   The paper does not\n",
      "propose solutions but serves as a call to action for the scientific community to\n",
      "prioritize the development of more robust watermarking techniques.\n",
      "<end_of_turn>\n",
      "Context items:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1,\n",
       "  'chunks': 'Visual paraphrasing is not yet a widely recognized sub-discipline. However, this paper demonstrates how visual paraphrasing can be accomplished using state-of-the-art text-to-image generation systems. This paper presents a critical assessment, empirically',\n",
       "  'chunk_char_count': 255,\n",
       "  'chunk_word_count': 31,\n",
       "  'chunk_token_count': 63.75,\n",
       "  'score': tensor(0.4268)},\n",
       " {'page_number': 4,\n",
       "  'chunks': 'Tree Ring Stable Signature Figure 5: This figure shows the variation of CMMD (Jayasumana et al.2024) and detectability of visual paraphrases with respect to strength and guidance scale. The images were watermarked using Tree Ring Watermarking (Wen et al.2023) and Stable Signature (Fernandez et al.2023). VS: We need justification of why we only benchmark these two and how they are representative of the broader class on watermarking methods. In lit review we mention SythiID and Zodiac but dont benchmark them?original image s=0.2 s=0.3 s=0.4 s=0.5 s=0.6 s=0.7 Prompt: Potrait of a Labrador in the style of Van Gogh Figure 6: Varying strength for content injection: The intensity of noise injected into the content is varied which impacts both the preservation of layout semantics and the fusion of prompt semantics.and consistency of digital images. In this procedure, the orig- inal context and content are preserved while producing an image that is semantically comparable.',\n",
       "  'chunk_char_count': 978,\n",
       "  'chunk_word_count': 152,\n",
       "  'chunk_token_count': 244.5,\n",
       "  'score': tensor(0.4060)},\n",
       " {'page_number': 1,\n",
       "  'chunks': 'This exemplifies the growing challenge of relying on visible markers for image authenticity in the context of rapidly advancing generative AI capabilities. Similarly, metadata consists of additional tags that can be easily stripped from files using a simple wrapper. Refer to a detailed example in the Appendix for further clarification. This paper exclusively critiques current techniques and empirically illustrates the deficiencies of state-of-the-art (SOTA) methods for AI-generated image detection. Rather than proposing a superior alternative method, this paper serves as a call to action for the scientific community to prioritize the development of more robust AI-generated im- age detection techniques. In this paper, our primary focus is on critiquing water- marking techniques. Although watermarking is primarily a technique originating from the computer vision community, there have been recent attempts to apply watermarking to AI- generated text. These endeavors have faced considerable criti- cism, primarily regarding the ease with which the watermarks can be removed using paraphrase attacks. This paper aligns with the philosophy that visual paraphrasing undermines the effectiveness of watermarking techniques. As such, we em- pirically demonstrate that visual paraphrasing can readily remove watermarks from images.',\n",
       "  'chunk_char_count': 1335,\n",
       "  'chunk_word_count': 187,\n",
       "  'chunk_token_count': 333.75,\n",
       "  'score': tensor(0.4044)},\n",
       " {'page_number': 2,\n",
       "  'chunks': 'demonstrating the vulnerability of existing watermarking techniques to visual paraphrase attacks. We do not propose any solutions to address this issue. Rather, this paper serves as a call to action for the scientific community to prioritize the development of more robust watermarking techniques. To the best of our knowledge, no prior work has empirically demonstrated the ability of visual paraphrase to de-watermark images. With this contribution, this paper establishes a scien- tific benchmark for future researchers interested in designing improved watermarking techniques for images generated by Generative AI. Exploring Related Works: Image Watermarking and Detection Methods Watermarking methods have emerged as prominent solu- tions for embedding and detecting ownership information within images. This section delves into the intricacies of these methodologies, exploring their respective strengths, limitations, and applicability in the context of AI-generated image detection. Watermarking Methods Watermarking techniques are broadly classified into two cat- egories: (i) static (i.e., non-learning) watermarking methods and (ii) learning-based watermarking methods. Static Watermarking Methods This method uses Discrete Wavelet Transform (DWT) (Lai and Tsai 2010) to decompose an image into several frequency sub-bands, applies Discrete Cosine Transform (DCT) (Yuan et al.2020) to each block of some of the sub-bands, and alters certain frequency coefficients of each block via adding a bit of the watermark.',\n",
       "  'chunk_char_count': 1523,\n",
       "  'chunk_word_count': 210,\n",
       "  'chunk_token_count': 380.75,\n",
       "  'score': tensor(0.4021)},\n",
       " {'page_number': 8,\n",
       "  'chunks': 'loss, this paper has underscored the critical role of techno- logical innovation in safeguarding the integrity of digital imagery. Moving forward, continued research and develop- ment in AI-generated image detection promise to fortify the foundations of a trustworthy digital ecosystem, empowering individuals and organizations to navigate the complexities of the information age with confidence and discernment. Ethical Considerations The development of visual paraphrasing methods, particularly those capable of dewatermarking state-of-the-art watermark- ing techniques, necessitates careful ethical considerations. While the primary intent behind this research is to advance the field of image processing and provide a deeper under- standing of watermarking resilience, it is crucial to acknowl- edge the potential for misuse. This study is conducted with the sole purpose of academic exploration and innovation, aiming to enhance the robustness of watermarking methods by identifying their vulnerabilities and improving overall se- curity. Our work is intended to contribute positively to the field, encouraging the development of more sophisticated and tamper-resistant watermarking techniques. Despite the benign intentions, the capabilities demon- strated by visual paraphrasing could be exploited for unethi- cal purposes, such as unauthorized removal of watermarks from copyrighted images, undermining the efforts of con- tent creators and rights holders to protect their intellectual property. To mitigate such risks, we emphasize responsible disclosure of our findings to stakeholders in the watermarking community, including researchers, developers, and content protection agencies, to foster collaborative improvements in watermarking technologies. Detailed methodologies and tools developed in this research will be restricted to aca- demic and professional entities with legitimate interests in advancing watermarking techniques. Furthermore, we advo- cate for the establishment of ethical guidelines for the use of visual paraphrasing tools, outlining acceptable uses such as research, education, and security testing, while explicitly prohibiting applications that infringe on intellectual property rights.',\n",
       "  'chunk_char_count': 2223,\n",
       "  'chunk_word_count': 295,\n",
       "  'chunk_token_count': 555.75,\n",
       "  'score': tensor(0.4015)}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = random.choice(query_list)\n",
    "query = \"What is the aim of this paper?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n",
    "print(f\"Context items:\")\n",
    "context_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashhar_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
